[{"path":"/2025/07/20/Adaptive kalman filtering algorithms for integrating GPS and low cost IN/","content":"Adaptive kalman filtering algorithms for integrating GPS and low cost INS 研究背景与问题 低成本惯性导航系统（INS）因传感器误差大且时变，难以直接用于高精度导航。传统卡尔曼滤波（CKF）采用固定的噪声矩阵，无法适应动态变化的噪声特性，导致初始化时间长、姿态对准精度低。本文提出三种自适应卡尔曼滤波算法，通过实时调整噪声模型提升GPS与INS集成的性能。 核心方法 过程噪声缩放（Process Noise Scaling） 原理：根据创新序列（INS预测与GPS观测的差异）动态缩放过程噪声协方差矩阵 (Q_k)，公式为： [P_{k}^{(-)} = P_{k-1}{(+)}{T} + S_{k}Q_{k-1}] 其中缩放因子 (S_k) 通过历史创新序列的方差比计算。 优势：计算简单，实时性强，显著缩短INS初始化时间。 多模型自适应估计（MMAE） 原理：并行运行多个不同噪声模型的卡尔曼滤波器，基于残差概率动态加权融合各滤波器输出。 实现：设置6个滤波器（(Q_k) 缩放因子从0.1到10000），引入最小概率阈值防止模型退化。 优势：适应性强，可处理复杂动态场景，但计算量较大。 自适应卡尔曼滤波器（AKF） 原理：通过状态修正序列 (x_k = K_k v_k^{(-)}) 估计过程噪声协方差，公式为： [*k {k}x_jx_j{T}] 局限：对未建模误差（如传感器轴偏差、振动）敏感，试验中表现不佳。 实验设计与结果 试验场景：英国普利茅斯海域小型测量船，配备低成本Crossbow MEMS IMU与高精度POS/MV系统（作为真值）。 数据采集：固定GPS模糊度，基线距离3km，记录10分钟初始化机动（8字转向）及后续直线航行数据。 关键结果 过程噪声缩放： 偏航角（Yaw）对准时间从CKF的10分钟以上缩短至275秒，误差降低30%（0.36° vs. 0.52°）。 横滚（Roll）与俯仰（Pitch）误差与CKF相当（0.04°）。 MMAE： 初始化时间303秒，偏航误差0.45°，略优于CKF但逊于过程噪声缩放。 动态场景下能自适应切换噪声模型（如转向时选择大缩放因子模型）。 AKF： 因未建模误差（如杠杆臂偏差）导致姿态误差大于CKF，需进一步优化参数估计策略。 结论与意义 有效性：过程噪声缩放与MMAE显著提升INS初始化效率及动态性能，适合低成本INS与GPS集成。 局限性：AKF需更精确的误差建模；MMAE计算复杂度高，需硬件并行支持。 应用价值：为低成本导航系统提供实用化解决方案，减少对高冗余硬件的依赖。 未来方向 增强算法鲁棒性，处理未建模误差（如传感器振动、轴偏差）。 优化计算效率，探索并行化或简化模型实现。 结合多传感器融合（如视觉、里程计）提升复杂环境适应性。 图表亮点 图4：创新序列显示初始化阶段误差达20cm，验证自适应算法的必要性。 图6：过程噪声缩放使偏航角快速收敛，CKF则持续波动。 图7：MMAE在机动阶段动态切换模型，体现其自适应能力。 本文通过理论创新与实证分析，为低成本导航系统的工程化提供了重要参考。"},{"path":"/2025/07/20/A Novel Adaptive Kalman Filter With Inaccurate Process and Measurement Noise Covariance Matrices/","content":"A Novel Adaptive Kalman Filter With Inaccurate Process and Measurement Noise Covariance Matrices 作者：Yulong Huang, Yonggang Zhang, Zhemin Wu, Ning Li, Jonathon Chambers 发表信息：IEEE Transactions on Automatic Control, 2018 1. 研究背景与问题 传统卡尔曼滤波（KF）依赖准确的噪声统计信息（过程噪声协方差矩阵 Qk 和测量噪声协方差矩阵 Rk），但在实际应用中，噪声统计常未知或时变，导致滤波性能下降甚至发散。现有自适应方法（如Sage-Husa AKF、IAKF、MMAKF）存在局限性： - Sage-Husa AKF：无法保证协方差收敛，可能发散； - IAKF：需大量数据窗口，不适用于快速时变噪声； - MMAKF：计算复杂度过高； - 现有VBAKF：仅能估计测量噪声协方差，假设过程噪声协方差准确。 本文提出一种新型变分贝叶斯自适应卡尔曼滤波（VBAKF），通过引入逆Wishart先验，联合估计状态、预测误差协方差矩阵（PECM）和测量噪声协方差矩阵（MNCM），解决PNCM和MNCM同时不准确的问题。 2. 核心方法 2.1 模型定义 考虑线性高斯状态空间模型： 目标：联合估计状态 xk、PECM Pk|k − 1 和 MNCM Rk。 2.2 先验分布选择 PECM先验：逆Wishart分布 参数通过名义PNCM Q̃k 和调节参数 τ 确定： 其中 。 MNCM先验：逆Wishart分布 参数通过遗忘因子 ρ 递推更新： 2.3 变分近似与迭代更新 通过变分贝叶斯方法，将联合后验分布近似分解为： p(xk, Pk|k − 1, Rk|z1 : k) ≈ q(xk)q(Pk|k − 1)q(Rk) 通过最小化Kullback-Leibler散度（KLD），交替更新各分量的参数： 更新PECM分布： 参数更新公式： $$ k^{(i+1)} = {k|k-1} + 1, _k^{(i+1)} = k^{(i)} + {k|k-1} $$ 其中 Ak(i) 为状态预测误差的期望协方差矩阵。 更新MNCM分布： 参数更新公式： 其中 Bk(i) 为测量残差的期望协方差矩阵。 更新状态分布： 通过修正的卡尔曼增益更新： 2.4 算法流程 预测步骤： 状态预测： PECM预测： 更新逆Wishart先验参数。 更新步骤： 固定点迭代（通常3-5次）： 计算修正的PECM 和 MNCM ； 更新卡尔曼增益 Kk(i + 1)； 更新状态估计 和协方差 Pk|k(i + 1)。 3. 参数选择与稳定性 调节参数 τ：控制先验PECM的权重，建议 τ ∈ [2, 6]； 遗忘因子 ρ：控制MNCM的时间波动性，建议 ρ ∈ [0.9, 1]； 名义PNCM Q̃k 和初始MNCM R̃0：需接近真实值以保证收敛； 迭代次数 N：通常 N ≥ 6 可保证收敛。 4. 仿真验证 4.1 实验设置 模型：二维目标跟踪，状态为位置和速度； 噪声：时变PNCM和MNCM（余弦调制）； 对比方法：KFNCM（固定名义协方差）、VBAKF-R（仅估计MNCM）、本文VBAKF。 4.2 结果分析 RMSE：VBAKF在位置和速度的RMSE分别比VBAKF-R降低54.5%和22.4%，接近真实协方差的KFTCM； 协方差估计：PECM和MNCM的SRNFN分别降低18.7%和60%； 计算效率：单步耗时 5.6 × 10−4 秒，高于VBAKF-R但精度显著提升。 5. 创新点与贡献 联合估计：首次在VB框架下同时估计状态、PECM和MNCM； 逆Wishart先验：利用共轭性简化后验分布形式； 鲁棒性：对PNCM和MNCM的不确定性具有强鲁棒性； 参数自适应：通过调节 τ 和 ρ 平衡先验与观测信息。 6. 局限与展望 计算复杂度：高于传统KF，需优化迭代次数； 先验依赖：名义PNCM需接近真实值； 扩展方向：非线性系统、多传感器融合、硬件加速。 参考文献： 文中引用包括经典KF理论、自适应滤波方法（Sage-Husa, IAKF）及变分贝叶斯相关研究（如Särkkä等），共25篇。"},{"path":"/2025/07/20/The interacting multiple model algorithm for systems with markovian switching coefficients/","content":"The interacting multiple model algorithm for systems with markovian switching coefficients 介绍的IMM——–为了克服系统可能会遇到不同类型的潜在故障，例如单个执行器故障、单个传感器故障以及同时传感器或执行器故障等，这些故障无法通过单个数学模型进行适当建模的问题，引入了由连续和离散机制驱动的随机混合系统。对于此类系统，最有效的 FDD 方法之一是基于交互多模型 （IMM） 方法，其中作一组模型匹配的卡尔曼滤波器 （KF） 来提供总体估计 1. 问题背景 研究目标是设计一种高效的滤波算法，用于如下线性系统： 系统模型： [x_t = a(i)x{t-1} + b(_i)w_t ] 观测模型： [y_t = h(_i)x_t + g(_i)v_t ] 其中： (_i) 是一个有限状态的马尔可夫链，取值于 ({1, , N})，其转移概率矩阵为 (H)。 (w_t) 和 (v_t) 是相互独立的高斯白噪声。 核心挑战：传统精确滤波算法（如卡尔曼滤波的扩展形式）的假设数量会随时间呈指数增长，导致计算复杂度不可行。 2. 现有方法的不足 假设管理问题：传统方法通过“剪枝”（pruning）或“合并”（merging）减少高斯假设数量，例如： 检测估计（DE）算法 广义伪贝叶斯（GPB）算法 缺点：在计算负载适中的情况下，性能不佳。 模型近似方法：如改进的多模型（MM）算法、修正增益扩展卡尔曼（MGEK）滤波等。 缺点：在多数场景中仍无法兼顾性能与计算效率。 3. 新思路：调整假设减少的时机 传统方法在测量更新后立即减少假设，但研究发现时机选择是关键创新点： IMM（交互多模型）算法：通过动态合并假设，优化时机选择，降低复杂度。 AFMM（自适应多模型遗忘）算法：通过剪枝减少假设数量。 优势： IMM算法通过合并而非剪枝，保留了更多有效信息，性能更优。 在动态多模型（MM）场景中，IMM的推导更简洁，计算效率更高。 4. 验证与结论 蒙特卡洛模拟：用于评估IMM算法的性能，验证其在多模型滤波领域的先进性。 意义：IMM算法通过优化假设管理时机，显著提升了滤波的实时性和准确性，成为该领域的重要进展。 关键术语解释 **马尔可夫链(_i)**：决定系统参数(a, b, h, g)的动态切换模式。 假设指数增长：每一步可能产生(N)个新假设（(N)为(_i)的状态数），导致(N^t)种可能路径。 剪枝 vs. 合并：剪枝直接删除低概率假设，合并则将相似假设融合为单一高斯分布。 总结 该研究针对马尔可夫跳变系统的滤波问题，提出通过优化假设管理时机（如IMM算法）来平衡计算复杂度和性能，解决了传统方法假设爆炸的难题，为实际工程应用提供了更高效的解决方案。"},{"path":"/2025/07/20/Risk-sensitive filtering for jump Markov linear systems/","content":"Risk-sensitive filtering for jump Markov linear systems 1. 系统模型与参考概率方法 1.1 系统动态方程 跳马尔可夫线性系统（JMLS）的状态与观测方程： $$ $$ - xk ∈ ℝn：连续状态，初始分布x0 ∼ 𝒩(x̄0, Σ0) - rk ∈ {e1, …, eN}：离散模态，服从转移矩阵Π的马尔可夫链 - wk ∼ 𝒩(0, In), vk ∼ 𝒩(0, Im)：独立高斯噪声 1.2 参考概率测度 定义参考测度，使得在下： - xk和yk独立且服从标准高斯分布 - 模态转移仍由Π控制 通过Radon-Nikodym导数调整测度： 其中λ̄l为调整因子，将原系统的噪声特性映射到参考测度下。 2. IMM滤波器的参考概率框架推导 2.1 未归一化密度函数 定义αkj(x)为在模态rk = ej下，状态xk的未归一化密度： 递推公式（定理1）： 解释：通过贝叶斯定理，将当前观测yk与前一时刻的状态z结合，考虑模态转移概率πij。 2.2 IMM近似 为避免指数爆炸，假设αkj(x)为单一高斯分布： αkj(x) = c̄kj𝒩(x; x̂k|kj, Σk|kj) 通过混合前一步的估计： - 解释：将多模型混合近似为单一高斯，以简化计算。 -– 3. 风险敏感滤波（RS-IMM）的扩展 3.1 风险敏感目标函数 定义风险敏感估计： - 核心思想：通过指数加权强调大估计误差，提升对模型不确定性的鲁棒性。 3.2 未归一化密度函数 定义γkj(x)为风险敏感框架下的未归一化密度： 递推公式（定理5）： - 关键区别：引入指数项exp (θ⋅)，放大历史误差的影响。 3.3 修改步骤（Modification Step） 混合估计的修正： Σ̃k − 1|k − 10j = [(Σk − 1|k − 10j)−1 − θQk − 1]−1 x̃k − 1|k − 10j = Σ̃k − 1|k − 10j[(Σk − 1|k − 10j)−1x̂k − 1|k − 10j + θQk − 1x̂k − 1|k − 1RS] - 物理意义：通过θQk − 1调整协方差，使估计偏向风险敏感的历史最优估计x̂RS。 -– 4. 最终估计计算 4.1 近似方案1（RS-IMM1） 假设近似为单一高斯分布： - 特点：直接继承IMM的混合权重，计算高效。 4.2 近似方案2（RS-IMM2） 通过泰勒展开exp (x) ≈ 1 + x，得到加权最小二乘解： 其中。 - 特点：显式考虑协方差修正，但计算更复杂。 -– 5. 性能对比与仿真结果 - IMM vs RS-IMM：在参数不确定时（如加速度模型失配），RS-IMM通过θ调节对历史误差的敏感度，显著降低速度估计误差（仿真中误差减少约10 m/s）。 - 收敛条件：需满足θ (Σk|kj)−1Qk−1，确保协方差修正后的正定性。 -– 总结 - IMM核心：多模型混合 + 高斯近似，平衡计算与精度。 - RS-IMM创新：引入指数代价函数，通过修改步骤动态调整估计偏向，提升鲁棒性。 - 工程意义：适用于模型参数不确定、噪声统计未知的场景，如目标跟踪中的强机动检测。 通过参考概率框架，论文统一了IMM与风险敏感滤波的推导，为跳马尔可夫系统提供了灵活的估计工具。"},{"path":"/2025/07/20/Recursive Noise Adaptive Kalman Filtering by Variational Bayesian Approximations  /","content":"Recursive Noise Adaptive Kalman Filtering by Variational Bayesian Approximations 1. 研究背景与问题 传统卡尔曼滤波（KF）及其扩展（EKF、UKF）要求噪声统计参数（如方差）完全已知，但实际应用中噪声参数常为时变或未知。传统自适应滤波方法（如状态增强、多模型IMM、粒子滤波）虽能解决此问题，但存在计算复杂度高或适用范围受限的缺陷。本文提出一种基于变分贝叶斯（Variational Bayesian, VB）框架的自适应卡尔曼滤波算法（VB-AKF），通过递归近似联合后验分布，实现状态与噪声参数的联合估计，兼顾计算效率与精度。 2. 核心方法 2.1 模型定义 考虑线性高斯状态空间模型： 其中，观测噪声协方差矩阵为对角阵 Σk = diag(σk, 12, …, σk, d2)，且噪声方差 σk, i2 为时变随机参数，服从独立动态模型。 2.2 变分贝叶斯近似 目标：近似联合后验分布 p(xk, Σk|y1 : k)。 假设后验分布可分解为状态与噪声方差的乘积形式： p(xk, Σk|y1 : k) ≈ Qx(xk)QΣ(Σk) 其中： Qx(xk) 为高斯分布 𝒩(xk|mk, Pk)； QΣ(Σk) 为独立逆伽马分布的乘积 。 通过最小化KL散度 KL[QxQΣ∥p(xk, Σk|y1 : k)]，推导出参数更新方程： - 状态更新：卡尔曼滤波公式 $$ $$ 噪声方差更新：逆伽马分布参数迭代 其中，Σ̂k = diag(βk, 1/αk, 1, …, βk, d/αk, d) 为噪声协方差的期望估计。 2.3 启发式方差动态模型 为保持后验分布形式，设计方差参数的动态模型： αk, i− = ρiαk − 1, i, βk, i− = ρiβk − 1, i 其中，ρi ∈ (0, 1] 控制方差的时间波动性（ρ = 1 表示方差恒定，ρ 1 允许方差随时间变化）。 2.4 VB-AKF算法流程 算法步骤： 1. 预测： - 状态预测：标准KF预测方程； - 方差预测：通过 αk, i− = ρiαk − 1, i 和 βk, i− = ρiβk − 1, i 更新。 更新： 初始化：mk(0) = mk−, Pk(0) = Pk−, , βk, i(0) = βk, i−； 定点迭代（通常2-3次）： 计算 Σ̂k(n) = diag(βk, i(n)/αk, i)； 更新状态参数 mk(n + 1), Pk(n + 1)； 更新方差参数 βk, i(n + 1)。 3. 实验验证 3.1 仿真模型 采用随机谐振器模型，观测噪声方差随时间突变（0.2 → 1 → 0.2），对比方法包括： - VB-AKF（ρ = 1 − e−4，迭代次数 N = 2）； - IMM算法（111个噪声模型，指数衰减模式转移概率）； - 标准KF（固定噪声方差）。 ##### 3.2 结果分析 - 估计精度：VB-AKF与IMM均能跟踪方差突变，IMM略优但计算成本高100倍； - 计算效率：VB-AKF耗时约为IMM的1%，且优于固定方差的KF； - 鲁棒性：VB-AKF对噪声动态模型假设不敏感，适用于低维测量场景。 4. 方法扩展与讨论 非线性扩展：通过替换KF为EKF或UKF，可推广至非线性系统； 与协方差匹配方法的联系：VB-AKF通过残差一致性估计噪声参数，与协方差匹配思想类似； 局限性：高维测量场景下计算复杂度仍可能较高，需进一步优化。 5. 总结与贡献 核心贡献：提出一种基于变分贝叶斯的自适应卡尔曼滤波框架，实现状态与噪声方差的联合递归估计； 优势：计算效率高（定点迭代收敛快）、适用于时变噪声、可扩展至非线性系统； 应用场景：低成本传感器融合（如GPS/INS）、故障容错系统等。 参考文献： 文中引用包括经典KF理论（Kalman, 1960）、自适应滤波（Mehra, 1972）、变分贝叶斯方法（Smidl Quinn, 2006）等，共25篇。"},{"path":"/2025/07/20/Linear matrix inequality optimization approach to exponential robust filtering for switched hopfield neural networks/","content":"Linear matrix inequality optimization approach to exponential robust filtering for switched hopfield neural networks 摘要 本文关注具有时滞的切换 Hopfield 神经网络的时滞相关指数鲁棒滤波问题。提出了一种新的时滞相关切换指数鲁棒滤波器，该滤波器导致具有保证鲁棒性能的指数稳定滤波误差系统。这些类型神经网络的切换指数鲁棒滤波器的设计可以通过求解线性矩阵不等式（LMI）来实现，这可以使用标准数值包轻松实现。给出了一个示例来说明所提出滤波器的有效性。 建立在先前研究的基础上 这些神经网络的指数稳定性在5中未进行研究。然而，本文在定理 3 的证明中展示了滤波误差系统的指数稳定性。 对早期声明的反驳 参考文献5讨论了权重矩阵的自适应学习问题。因此，[参考文献5]中的结果不能应用于滤波问题。 因此，5中的结果不能应用于滤波问题。与5中的结果相比，本文考虑了切换 Hopfield 神经网络的滤波问题。 在论文中，我们提出了一种新的切换指数 H∞滤波器用于这些神经网络。论文中提出的滤波器形式、LMI 条件、Lyapunov-Krasovskii 泛函、指数稳定性以及应用实例与文献5中的不同。 未来工作 减少LMI保守性（如结合自由权矩阵法）； 离散时间SHNN的滤波设计； 多时滞与随机切换场景的扩展。 关键方法与理论贡献 滤波器设计 结构：基于切换信号的多模态滤波器，形式为： x˙(t)=∑i=1Nξi(t)[Aix(t)+Wiϕ(x(t−τ))+L(y(t)−y^(t))]x˙(t)=i=1∑*N**ξi(t*)[Aix^(t)+Wiϕ(x(t−τ))+L(y(t)−y(t))] 其中LL为待设计的滤波器增益矩阵。 目标：通过增益LL使滤波误差e(t)=x(t)−x(t)e(t)=x(t)−x(t)满足指数稳定性和H∞性能。 Lyapunov-Krasovskii泛函与LMI条件 构造新型泛函：结合时滞项与指数权重项，形式为： V(t)=exp⁡(κt)eT(t)Pe(t)+积分项（含时滞状态与干扰）V(t)=exp(κt)eT(t)Pe(t)+积分项（含时滞状态与干扰） 稳定性分析：通过Jensen不等式和矩阵变换，导出保证指数稳定性和H∞性能的LMI条件： [块矩阵（含时滞、增益、干扰项）]0[块矩阵（含时滞、增益、干扰项）]0 滤波器增益求解：通过LMI可行解M=PLM=PL，反推L=P−1ML=P−1M。 理论优势 时滞相关：直接处理时滞影响，避免保守性； 指数稳定性：误差以指数速率收敛； H∞性能：干扰衰减水平γγ可调。 关键点 这篇论文的局限性包括 LMI 条件的潜在保守性，如果κ和γ分别被选为较大和较小，这种保守性可能会增加。 未来工作包括通过使用增强的 Lyapunov-Krasovskii 泛函和自由加权矩阵方法来减少 LMI 条件的保守性。 这项研究的实际应用领域包括具有时延的切换 Hopfield 神经网络的估计与控制。所提出的滤波器可用于多个领域，如机械系统控制、汽车工业、开关电源转换器以及其他许多领域。 这篇论文的实际应用包括设计具有时间延迟的切换 Hopfield 神经网络的滤波器，这些滤波器可用于信号处理和控制系统等各个领域。 关键概念 #指数稳定性; #时间延迟; #神经网络; #线性矩阵不等式; #声明/滤波问题; #滤波问题 引用 本文提出了一种新的切换指数 H∞滤波器，用于具有时间延迟的切换 Hopfield 神经网络，该滤波器保证了滤波误差系统的指数稳定性，并具有指定的 H∞性能水平。 关键发现 该研究的关键发现是开发了一种针对具有时延的切换 Hopfield 神经网络的指数 H∞滤波器设计的方法，以及提出了一种新的可通过对线性矩阵不等式（LMI）进行求解来设计的时延相关指数 H∞滤波器。该滤波器被证明是指数稳定的，并保证了 H∞性能。 本文的主要发现包括开发了一种新的切换指数 H∞滤波器、构建了一种新的李雅普诺夫-克拉索夫斯基泛函以及通过求解一个时延相关的 LMI 来确定滤波器增益矩阵。 我们已通过开发一种新型指数 H∞滤波器来解决具有时延的切换 Hopfield 神经网络的指数 H∞滤波问题 新 Lyapunov–Krasovskii 函数的正确构造表明，滤波误差系统具有保证的 H∞性能，并呈指数稳定 提出的成果预期可扩展到离散时间切换 Hopfield 神经网络 目标 本研究的目的是设计一个在 H∞意义上保证性能的适合于切换 Hopfield 神经网络的滤波器，具体来说，要找到一个合适的滤波器，使得滤波误差系统是指数稳定的，并且具有保证的 H∞性能。 方法 本研究采用的方法包括构建一个新的李雅普诺夫-克拉索夫斯基泛函，以及使用线性矩阵不等式（LMI）来设计滤波器。通过求解线性矩阵不等式（LMI），可以实现所需切换指数 H∞滤波器的设计，这可以通过标准数值算法轻松实现。 本文采用的方法包括构建一个新的李雅普诺夫-克拉索夫斯基泛函、通过求解延迟相关的 LMI 确定滤波器增益矩阵，以及使用 MATLAB LMI 控制工具箱求解 LMI。 数值仿真与验证 模型参数：双模态SHNN，时滞τ=1τ=1，非线性激活函数为双曲正切，外部干扰为高斯噪声。 LMI求解：通过MATLAB LMI工具箱，设定γ=0.4γ=0.4，获得滤波器增益： L=[0.98910.5127]L=[0.98910.5127] 仿真结果： 图1-2：真实状态x1(t)x1(t)、x2(t)x2(t)与估计值x1(t)x1(t)、x2(t)x2(t)快速收敛； 图3：滤波误差e(t)e(t)在2秒内趋于稳定，验证指数稳定性； 标准差分析：γ=0.4γ=0.4时误差标准差为0.6483（e1e1）和0.4505（e2e2），增大γγ至1.5时性能下降，符合理论预期。 结果 该研究的结果表明，所提出的滤波器具有指数稳定性，并保证 H∞性能。滤波器可以通过求解线性矩阵不等式（LMI）来设计，滤波器的增益矩阵可以表示为 L = P^(-1)M。 本文结果表明，所提出的滤波器保证了滤波误差系统的指数稳定性，并具有指定的 H∞性能水平，滤波器增益矩阵可以通过求解一个时延相关的 LMI 来确定。 发现 本文提出的成果预期可扩展到离散时间切换 Hopfield 神经网络 局限性 这篇论文的局限性包括 LMI 条件的潜在保守性，如果κ和γ分别被选为较大和较小，这种保守性可能会增加 LMI条件可能保守（未引入自由权矩阵）； 未扩展至离散时间系统。 结论 本研究的结论是，所提出的基于延迟的指数 H∞滤波器对于具有时延的切换 Hopfield 神经网络是有效的，并且可以通过求解线性矩阵不等式（LMI）来设计该滤波器。该滤波器具有保证的 H∞性能和指数稳定性。 本文结论是，所提出的切换指数 H∞滤波器在保证滤波误差系统的指数稳定性以及满足特定的 H∞性能水平方面是有效的，并且可以通过求解一个时延相关的 LMI 来确定滤波器的增益矩阵 参考文献 [1] Hopfield, J.J.: Neurons with grade response have collective computational properties like those of a two-state neurons. Proc. Natl. Acad. Sci. 81, 3088–3092 (1984) OA GScholar Scite [2] Gupta, M.M., Jin, L., Homma, N.: Static and Dynamic Neural Networks. Wiley-Interscience, New York (2003) OA GScholar [3] Huang, H., Qu, Y., Li, H.: Robust stability analysis of switched Hopfield neural networks with timevarying delay under uncertainty. Phys. Lett. A 345, 345–354 (2005) OA GScholar Scite [4] Lou, X.Y., Cui, B.T.: Delay-dependent criteria for robust stability of uncertain switched Hopfield neural networks. Int. J. Autom. Comput. 4, 304–314 (2007) OA GScholar Scite [5] Ahn, C.K.: An H∞ approach to stability analysis of switched Hopfield neural networks with timedelay. Nonlinear Dyn. 60, 703–711 (2010) OA GScholar Scite [6] Wang, Z., Ho, D.W.C., Liu, X.: State estimation for delayed neural networks. IEEE Trans. Neural Netw. 16, 279–284 (2005) OA GScholar Scite [7] He, Y., Wang, Q.G., Wu, M., Lin, C.: Delay-dependent state estimation for delayed neural networks. IEEE Trans. Neural Netw. 17, 1077–1081 (2006) OA GScholar Scite [8] Liu, Y., Wang, Z., Liu, X.: Design of exponential state estimators for neural networks with mixed time delays. Phys. Lett. A 364, 401–412 (2007) OA GScholar Scite [9] Wang, Z., Liu, Y., Liu, X.: State estimation for jumping recurrent neural networks with discrete and distributed delays. Neural Netw. 22, 41–48 (2009) OA GScholar Scite [10] Stoorvogel, A.: The H∞ Control Problem: A State-Space Approach. Prentice Hall, London (1992) OA GScholar [11] Huang, H., Feng, G.: Delay-dependent H∞ and generalized H2 filtering for delayed neural networks. IEEE Trans. Circuits Syst. I, Fundam. Theory Appl. 56, 846–857 (2009) OA GScholar Scite [12] Boyd, S., Ghaoui, L.E., Feron, E., Balakrishinan, V.: Linear Matrix Inequalities in Systems and Control Theory. SIAM, Philadelphia (1994) OA GScholar [13] Gahinet, P., Nemirovski, A., Laub, A.J., Chilali, M.: LMI Control Toolbox. The MathWorks, Inc., Natik (1995) OA GScholar Scite [14] Narendra, K.S., Tripathi, S.S.: Identification and optimization of aircraft dynamics. J. Aircr. 10, 193– 199 (1973) OA GScholar Scite [15] Noldus, E.: Stabilization of a class of distributional convolutional equations. Int. J. Control 41, 947– OA GScholar Scite [16] He, Y., Wu, M., She, J.H., Liu, G.P.: Delay-dependent robust stability criteria for uncertain neutral systems with mixed delays. Syst. Control Lett. 51, 57–65 (2004) OA GScholar Scite [17] Wu, M., He, Y., She, J.H., Liu, G.P.: Delay-dependent criteria for robust stability of time-varying delay systems. Automatica 40, 1435–1439 (2004) OA GScholar Scite"},{"path":"/2025/07/19/Latent-KalmanNet Learned kalman filtering for tracking from high dimensional signals/","content":"Latent-KalmanNet: Learned kalman filtering for tracking from high-dimensional signals 本文的方法建立在KalmanNet架构之上，所提出的Latent - KalmanNet旨在解决在部分已知动态下从复杂的高维观测值中进行跟踪的挑战。它利用数据实现可靠的跟踪，克服了测量函数和噪声统计知识的缺失。本文提出的Latent - KalmanNet算法不局限于使用瞬时估计量进行潜在特征提取，实际上可以学习到对潜在状态编码的贡献。此外，可以利用其对状态演化模型的访问来跟踪部分可观测环境下的状态。 1. 问题描述 1.1 SS模型 yt可以用于恢复xt中p ≤ m，给定一个p × 1的向量PXT，其中P是一个p × m的选择矩阵。 该论文考虑 部分可观测设置，因为可以通过令P = I和p = m使得模型的状态全部可观测，并且假设状态方差可以从yt中得到，P给出已知，即对于当前系统具有可观察到的状态变量的所有信息。 1.2 本文目标及挑战 设计一种实时状态估计的过滤算法，可应用于完全或部分可观测的SS模型，给定估计器的性能指标，x̂t使用均方误差(MSE)，定义为： E{||X̂T − XT||2} 主要处理以下四种情况： 挑战 内容 C.1 et和vt分布未知，可能为非高斯分布 C.2 使用的状态转换函数f(⋅)可能不匹配 C.3 观测维度维度高，n ≫ m，导致高复杂性影响实用性 C.4 测量函数h(⋅)未知，并且可能是非初级的。 利用标记好的数据集 训练，该轨迹由长度为 的观测值和测量值组成： 2. 算法介绍 2.1 EKF 是最常见的滤波算法之一，适用于噪声为高斯且SS模型完全已知的情况，整体框架如下： 预测步骤 状态估计更新（同EKF）： 其中， 其中： 矩阵F̂t和Ĥt分别是f(⋅)和h(⋅)的瞬时线性化： F̂t = ∇xf(x̂t − 1); Ĥt = ∇xh(x̂t|t − 1). 2.2 KalmanNet 详细见《KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics》: https://otreatly.github.io/2025/07/18/KalmanNet/https://otreatly.github.io/2025/07/18/KalmanNet/ 通过让gθKG表示带有参数θ的RNN映射，将KG设置为： 𝒦t(θ) = gθKG(Δyt, Δxt − 1) 从数据集𝒟中学习RNN的权重θ，其损失函数为： ℒ = ||x̂t − xt||+λ||θ||2 2.3 Latent-KalmanNet Step 1 - 瞬时估计 用带有参数ψ的递归DNN，记作$g_{}e:n ^p ，训练其将y_t映射为可观测的状态变量P_{X_t}的估计。参数可以基于梯度优化，以监督的方式进行学习，给定正则化范数MSE损失：$ ^e_() = ^{|D|}{d=1} ^T{t=1} ||ge_ψ(y{(d)}_t) −z^{(d)}_t||^2 +λ ∥ψ∥^2 $$ Step 2 - 整合演化模型 瞬时估计忽略了部分已知的状态演化模型，由于时间相关性，将状态演化函数f(⋅)引入，潜在改进gψe，提供的状态变量估计。 如下一个高层框架： 先验估计xt|t − 1作为DNN瞬时估计的额外特征，基于DNN的估计器采用两个多变量输入x̂t|t − 1和yt（从ℝn × ℝm映射到ℝp)： zt = gψe(yt, x̂t|t − 1) Step 3 - 瞬时估计 假设估计器输出zt和可观测状态变量PXt之间服从高斯分布，这种方法允许编码器的输出进行更总，而不需考虑C1和C2，可以通过敬爱嗯EKF与Step 2中的预训练DNN编码器级联应用来解释时间相关性。其中假设DNN经过适当的训练，使得估计值yt逼近PXt的最小MSE估计。其中DNN的输出可以近似为： zt = gψe(yt, x̂t|t − 1) ≈ Pxt + ṽt 其中，ṽt为零均值且与xt相互独立，如果其也是高斯且和时间独立，那么该系统就表示一个高斯SS模型（可能非线性），可以使用EKF对xt进行追踪。而KG计算中所需的vt二阶矩可以从DNN编码器的验证误差中估计出，这种情况下，测量矩阵Ĥ(t) = P，系统如下所示： Step 4 - Laten-KalmanNet 框架 上述Step 3介绍了系统基于编码器输出zt和状态xt的关系服从SS模型，考虑到误差项可能服从未知分布的事实，选择使用KalmanNet而不是EKF，本文使用的Latent-KalmanNet绕过了SS模型对噪声所需特定分布的需要，其详细结构见下： 关于xt的估计可以写作： x̂t = gθf(zt, x̂t − 1) 编码器和解码器( KalmanNet )通过提供一个低维的潜在表示(通过编码器)和一个获得潜在表示的先验(通过KalmanNet )来相互辅助。下面详细的训练过程鼓励整个编码器解码器的潜在表示zt是导致KalmanNet输出的最准确的状态估计的一种，而不一定是Pxt (如第1 - 3步)的最可靠的估计。一旦训练好，在推理过程中，每个时间步上的估计过程被总结为算法1。 单步Latent-KalmanNet流程 在每个时间步上不涉及雅可比计算或矩阵求逆。这表明，与其他基于模型的解决方案以及具有大量权重的数据驱动方法相比，Latent-Kalman Net非常适合应用于高维SS模型和计算受限的设备。 训练 所提出框架由两个模块串联：DNN估计器gψe(⋅)和KalmanNetgθf(⋅)。两者均可微分，能够由(θ, ψ)参数化的整体架构作为判别模型进行端到端的训练。 数据集𝒟的评价标准基于损失反响传播到KG 𝒦t计算的能力，评价标准如下： 其中λ1, λ2 0，并且： ℒt(d)(θ, ψ) = ∥x̂d(d) − xt(d)∥2 这MSE损失基于KalmanNet输出而不是gψe(⋅)的输出计算，意味着不需要学习估计可观测状态变量PXt。 模型最终的输出如下： Misplaced & \\hat{x}_t^{(d)} = g_{\\theta}^f\\left( g_{\\psi}^{e}\\left( y_t^{(d)},f(\\hat{x}_{t-1}^{d}) \\right) ,\\hat{x}_{t-1}^{(d)} \\right)\\\\ =\\hat{x_{t|t-1}^{d}+\\mathcal{K}_t(\\theta)\\cdot (z_t^{(d)}-\\hat{z}_{t|t-1}^{(d)})} 可以通过KalmanNet的输出x̂t(d)中得到给定轨迹d在给定时间步长t下相对于KG的损失梯度： 其中Δxt = xt(d) − x̂t|t − 1(d), Δzt = zt(d) − ẑt|t − 1(d)。 基于其可以将高纬观测数据yt（包括先验x̂t|t − 1）作为隐性特征输入，提出了Latent-KalmanNet： 3. 仿真优化 3.1 部分可观测的动态系统 针对一个摆进行研究： 3.1.1 SS Modle 状态包含角度ϕt和角速度wt： xt = [ϕt, wt]T 摆的状态演化过程由机械系统定律定义，本质为高度非线性的，其方程如下： 其中，零均值高斯噪声的协方差Q = q2 ⋅ I，q2 = 0.1，重力加速度取g = 9.81[m/sec2]，弦长用l表示。观测数据yt是摆的采样轨迹生成的28 × 28的灰度照片。高斯噪声vt协方差R = r2 ⋅ I, r2 ∈ 0.001, …, 0.25，此系统只能从图片中观测到角度，为部分可观测模型，P = [1, 0] 3.1.2 追踪方法 Step 1：编码器 在数据集上以监督的方式进行训练，使用损失函数和Adam优化器将每个yt映射为可观测状态变量ϕt。 Step 2：编码+先验 如上文中提到的整合架构 Step 3：编码+先验+EKF 应用EKF，测量函数设置为h(x) = Px，通过网格搜索选择状态演化噪声的方差，观测噪声由编码器输出端的经验估计损失决定。 Step 4：Latent-KalmanNet 使用的前面的Step 2，并于KalmanNet提出的框架2相结合。 3.2 Lorenz吸引子混沌系统 3.2.1 SS Modle 对非线性Lorenz系统的SS模型下的各场景进行仿真，从而对Latent-KalmanNty及其MSE和延迟方面进行性能评估。由微分方程得到无噪声的演化方程: 通过采样时间间隔Δt对无噪声过程进行采样，并假设其在小范围领域内保持为常数，即： A(xt) ≈ A(xt + Δt) 对于上述微分方程在连续时间的解为： xt + Δt = exp(A(xt) ⋅ Δt) ⋅ xt 进行泰勒展开： 得到如下离散时间演化过程： xt + 1 = f(xt) = F(xt) ⋅ xt. 状态x = [x1, x2, x3]T在坐标c ∈ ℝ2上评估的测量函数被建立为高斯点扩散函数 3.2.2 训练方法 编码器 同3.1.2，三个卷积层和两个P=3输出神经元的FC层 编码器+先验 编码器+先验+EKF 应用EKF，即H(x) = x，利用贪婪搜索选择演化噪声的差异，观测噪声由MSE在编码器输出处确认； RKN Latent-KalmanNet 同3.2.1 问题 1.为什么使用RNN而不是transform等其他结构？ transform是非常高度的参数化体系结构。我们基于模型的深度学习设计中的指导原则是将小型的，适应性的组件集成到传统的算法管道中，尤其是在缺乏域知识的情况下。减少参数化有助于存储限制，功耗，并有助于在移动设备和边缘设备上实现实时推理功能。这些目标需要最大程度地减少神经网络中的参数数量。 并且transform专注于在时间序列中利用远程相关性，这是他们在自然语言处理中取得巨大成功的原因之一。当我们专注于在SS模型中跟踪时，该模型封装了马尔可夫结构，从而利用RNNS完成的处理中的最新记忆将是足够的。"},{"title":"notes","path":"/2025/07/18/index/","content":"sssdadasda"},{"path":"/2025/07/18/test/","content":""},{"path":"/2025/07/18/Linux工具/","content":"Linux工具 1.vscod 应用中心直接下载安装 插件： chinese vim python codesnap docker git graph jupyter project 2.typora 2.1下载安装typora wget -qO - https://typoraio.cn/linux/public-key.asc | sudo tee /etc/apt/trusted.gpg.d/typora.asc sudo add-apt-repository 'deb https://typoraio.cn/linux ./' sudo apt-get update sudo apt-get install typora 安装完成： 2.2配置激活 利用git克隆’Yporaject’ sudo apt install gitgit clone git@github.com:hazukieq/Yporaject.git sudo apt install cargo cd Yporaject/ cargo build 克隆完成 查看是否有所需文件 ls target/debug##看看结果有没有 node_injectcargo runsudo cp target/debug/node_inject /usr/share/typora 新开一个终端执行下面代码： cd /usr/share/typora/sudo chmod 777 node_injectsudo ./node_inject ##下方将打印就对啦extracting node_modules.asaradding hook.jsapplying patchpacking node_modules.asardone!## 返回前面终端获取激活码 cd license-gen/cargo buildcargo run 激活码如下： 进入typora利用上面得到的激活码进行激活，等待一段时间选择确定 激活成功 2.3问题 突然出现许可证失败 个人解决方案： 卸载现在版本： sudo apt remove typora#删除其配置可用#sudo apt purge typorarm -rf ~/.config/typorarm -rf ~/.cache/typora 下载低版本（成功案例为1.8.10） wget https://download2.typoraio.cn/linux/typora_1.8.10_amd64.debsudo dpkg -i typora_1.8.10_amd64.deb 重新执行前面的安装步骤，但是在使用cargo build之前，要清除之前创建的缓存记录（重中之重）： cargo cleancargo update 3.zotero 3.1下载及安装 https://www.zotero.org/download/ 解压完成进入下载/Zotero-7.0.12_linux-x86_64打开终端。 创建目录 sudo mkdir /opt/zotero 将解压后的文件移至与浏览器同目录下/opt/ # Zotero_linux-x86_64是解压的zotero所有sudo mv Zotero_linux-x86_64/* /opt/zotero/ 更新zotero桌面位置 cd /opt/zoterosudo ./set_launcher_icon 执行文件即可打开 ./zotero 生成快捷图表 sudo apt install vimsudo vim zotero.desktop##下面是需要修改的文件内容[Desktop Entry]Name=ZoteroExec=bash /opt/zotero/zotero //这里把执行路径改为绝对路径Icon=/opt/zotero/chrome/icons/default/default256.pngType=ApplicationTerminal=falseCategories=Office;MimeType=text/plain ln -s /opt/zotero/zotero.desktop ~/.local/share/applications/zotero.desktop 3.2数据同步 4.坚果云 方法一 https://www.jianguoyun.com/s/downloads/linux下载安装包 方法二 deb包安装及解决依赖方法 使用gdebi安装，自动安装依赖 sudo apt isntall gdebisudo gdebi nautilus_nutstore_amd64.deb ubuntu双击使用软件中心安装，自动安装依赖 使用dpkg安装，使用apt-get -f install安装依赖 sudo dpkg -i nautilus_nutstore_amd64.debsudo apt-get install -f 5.edge 方法一 官网直接下载https://www.microsoft.com/zh-cn/edge/?form=MA13FJ，解压即可安装： sudo dpkg -i microsoft-edge-stable_133.0.3065.69-1_amd64.deb 方法二 ubuntu的安装命令： ## Setupcurl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor microsoft.gpgsudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/sudo sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/edge stable main\" /etc/apt/sources.list.d/microsoft-edge-dev.list'sudo rm microsoft.gpg## Installsudo apt updatesudo apt install microsoft-edge-dev 6.QQ 5.1下载安装包 选择合适的.deb版本文件https://im.qq.com/linuxqq/index.shtml 5.2安装 sudo dpkg -i example.deb 7.anaconda 6.1下载及安装 下载链接 https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh 到安装包所在路径打开终端 添加权限 chmod +x Anaconda3-2024.10-1-Linux-x86_64.sh 运行.sh脚本 ./Anaconda3-2024.10-1-Linux-x86_64.sh 按q跳过，选择yes 选择默认安装地址： anaconda默认安装地址 6.2问题 终端未找到conda指令 终端输入以下命令编辑bash配置文件 nano ~/.bashrc 最后一行加入地址 export PATH=/home/用户名/anaconda3/bin:$PATH ctrl+X 保存退出 重置配置文件使其生效 source ~./bashrc 关闭终端默认打开base环境 conda config --set auto_activate_base false"},{"path":"/2025/07/18/KalmanNet/","content":"KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics 针对已知部分动态系统的神经网络辅助的卡尔曼滤波。”作者将RNN(recurrent neural network)和MB-KF(model based kalman filter)进行结合，提出了一种新的滤波方案KalmanNet，具体来说就是在每一时刻通过RNN预测卡尔曼增益K，其余操作与传统卡尔曼滤波五个公式保持一致。 1. 研究背景与问题 核心问题：传统卡尔曼滤波（KF）及其非线性变体（EKF、UKF）依赖精确的状态空间（SS）模型和噪声统计特性（Q,R），而实际系统中模型常存在非线性或部分未知（如噪声统计未知、动态方程近似）。 现有方法局限： 模型驱动（MB）方法：对模型失配敏感，鲁棒性差。 数据驱动（DD）方法（如RNN）：需大量数据和参数，缺乏可解释性，计算复杂度高。 目标：提出KalmanNet——融合MB与DD的混合框架，在部分已知动态下实现高效、鲁棒的状态估计。 2. KalmanNet 2.1 MB-KF 针对线性系统的经典卡尔曼滤波公式总结如下： ​ 预测 ​ 更新 2.2 System Model 该篇文章考虑的系统模型如下式4： f(⋅)和h(⋅)表示可能的非线性函数。 2.3 KalmanNet 假设动态系统部分已知，及式4存在模型失配，且噪声Q和R未知，KalmanNet不依赖噪声分布，无需对二阶矩进行量式计算，通过RNN固有内存允许隐式跟踪二阶统计矩Q和R，而Kt通过RNN从数据中学习。 2.3.1 整体架构 设计思想：保留KF的递归结构，用RNN替代对噪声统计敏感的卡尔曼增益（KG） 计算模块。 关键公式： 预测步骤 状态估计更新（同EKF）： KG计算：原KF中 Double subscripts: use braces to clarify K_k = _k H^T_k (H_k _k H^T_k + R_k)^{-1} ，现由RNN学习。 2.3.2 RNN输入特征设计 选取与噪声统计相关的时序特征作为RNN输入： F1: 观测差值 Δỹ = yt − yt − 1; F2: 新息（Innovation）Δyt = yt − ŷt ∣ t − 1; F3: 状态后验差值 Δx̂t = x̂t ∣ t − x̂t − 1 ∣ t − 1; F4: 状态更新差值 Δx̂t = x̂t ∣ t − x̂t ∣ t − 1; 最佳组合：实验表明 {F1, F2, F4} 或 {F1, F3, F4} 效果最优。 2.3.3 RNN架构设计 架构#1（隐式跟踪统计矩）： 结构：全连接层 → GRU层 → 输出层（生成 Kt ∈ Rm × n）。 特点：参数量大（GRU隐藏层维度为 10(m2 + n2)），但灵活性高。 在MB-KF中的噪声特性主要表现为每一时刻的卡尔曼增益K，该方法直接训练递归神经网络RNN网络对卡尔曼增益k进行预测，替代增益计算步骤，其结构图如下所示： 架构#2（显式分解统计矩）： 结构：三个独立GRU分别跟踪 Q、Σt ∣ t − 1、St，按KF公式计算KG。 特点：参数量少（约2.5e4 vs. 5e5），可解释性强，但灵活性较低。 2.3.4 GRU(门控循环单元) 更行门决定了如何将新的输入信息与之前的记忆相结合 rt = σWt ⋅ [ht − 1, xt] 相关性门 → 定义前面记忆保存到当前时间步的量 h̃t = tanh(W ⋅ [rt * ht − 1, xt]) 节点状态 ht = (1 − zt) * ht − 1 + zt * h̃t 隐层输出 输出 当相关性门rt = 1，更新门zt = 0，就是标准的RNN模型 2.3.5 训练算法 损失函数：状态估计的均方误差（MSE）： 梯度计算：通过KG反向传播（链式法则）： 监督学习：利用标记数据集训练： 其中，N为轨迹数量。 训练策略： V1：标准BPTT（全轨迹训练）。 V2：截断BPTT（长轨迹分块，块内训练）。 V3：固定短轨迹训练（适用于快速收敛到稳态的系统，如线性SS模型）。 推荐：先用V2预热，再用V1微调。 优点 RNN从数据中学习，计算卡尔曼增益K时不依赖与噪声特性的先验知识，不需要计算Q和R，避免了矩阵求逆； KalmanNet中的滤波不依赖于序列长度； KalmanNet对于模型失配和非线性的SS模型，性能优于其他滤波方法。 缺点 需要标注数据进行训练； 3. 实验设计与结果 3.1 实验设置 评估指标：状态估计MSE（dB）。 Baseline： MB方法：KF、EKF、UKF、PF DD方法：Vanilla RNN、MB-RNN（融合部分模型） KalmanNet配置： C1: 架构#1 + {F2,F4} + V3训练 C2: 架构#1 + {F2,F4} + V1训练 C3: 架构#1 + {F1,F3,F4} + V2训练 C4: 架构#2 + 全特征 + V1训练 3.2 仿真 3.2.1线性状态空间模型（Linear State Space Model） state dimension: m observation dimension: n 状态演变矩阵F 观测矩阵H 观测纬度为2时，旋转不同角度α，得到Fα∘和Hα∘。 3.2.2 线性运动学模型（linear kinematic model） 考虑恒定加速度模型(Constant Acceleration Model,CA)和恒速模型（Constant Velocity Modle，CV），全信息为P, V, A,非全信息模型只知道位置状态P dim of state for CA model: m = 3 dim of state for CV mode: mcv = 2​ 时间步长： Δtgen = 1 × 10−2 状态演变矩阵： 观测矩阵： 齐次非齐次 噪声设置： 为缩放因子 3.2.3 Lorenz Atractor state dimension: m = 3 observation dimension: n = 3 时间步长： 泰勒展开项： 精确模型不精确模型 3.3 关键实验结果 场景 结论 线性SS模型（全信息） KalmanNet（C1）达到KF的MMSE界，且可迁移至长轨迹（训练 (T=20)，测试 (T=200)）。 线性SS模型（部分信息） 在状态演化矩阵 () 旋转失配（(^)）时，KalmanNet（C2）较KF提升3 dB。 非线性SS模型（正弦+多项式） 在噪声水平高时，KalmanNet（C4）超越EKF（图7）；模型失配下仍接近全信息性能（表IV）。 Lorenz吸引子（混沌系统） 在采样失配（连续→离散）时，KalmanNet（C4）MSE = -11.28 dB，显著优于EKF (-6.43 dB) 和PF (-5.34 dB)（表IX）。 真实数据（NCLT数据集） 里程计定位任务中，KalmanNet（C1）MSE = 22.2 dB，优于EKF (25.39 dB) 和RNN (40.21 dB)（表X），有效抑制漂移（图11）。"},{"title":"关于","path":"/about/index.html","content":"友链关于昵称简介 关于本站 本站没有任何推广和打赏链接，如果您觉得哪个作品不错，欢迎去对应的仓库点个赞，或者在对应的文章下面留言互动一下。 开源项目无任何盈利目的，只在工作闲暇时间进行维护，有相关需求请前往对应项目提 Issue 进行反馈，通过私人邮件询问开源项目问题可能得不到答复。"},{"title":"收藏","path":"/bookmark/index.html","content":"…"},{"title":"探索","path":"/explore/index.html","content":"…"},{"title":"DropOut","path":"/wiki/数据同化与不确定性量化/Dropout.html","content":"前言 深度神经网络包含多个非线性隐藏层，这使得它们有强大的表现力，可以学习输入和输出之间非常复杂的关系。但是在训练数据有限的情况下，深度神经网络很容易过度学习造成过拟合，过拟合是深度神经网络的一个非常严重的问题，此外，神经网络越大，训练速度越慢，Dropout可以通过在训练神经网络期间按照一定的概率将神经网络单元暂时从网络中丢弃来防止过拟合。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。实验证明Dropout有很好的效果。 1. Dropout简介 1.1 图解 左图(a)：拥有两层隐藏层的正常升级网络；右图(b)：应用Dropout后的神经网络；源自官方论文 1.2 Dropout产生动机 官方论文中就dropout的动机做了一个类比案例，以一次恐怖袭击为例，使用下述两种方式： 集中50人，让50人精密分工，搞一次打爆破； 将50人分成10组，每组5人，分头行动。随便搞些动作，成功一次就算； 从成功概率上看，明显后者更容易成功。将这观念使用到深度学习中，神经网络的各个隐藏单元在每次训练中，存在不同的组合搭配，每个mini-batch都是不同的网络，使其它隐藏层神经网络单元变得不可靠，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，从而阻止了共适应的发生。一个隐藏层神经元不能依赖其它特定神经元去纠正其错误，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。 1.3 流程简介 在训练过程中，每一次迭代，随机将一些神经元临时丢弃，进行完此次训练和优化后，在下一次迭代过程，再次随机将一些神经元临时丢弃，不断重复上述步骤至训练结束。 整体步骤如下 随机（临时）删掉网络中一半的隐藏神经元（以dropout rate为0.5为例），输入输出神经元保持不变。 把输入x通过修改后的网络 前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批（batch_size自定义）训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数(w, b) 重复以下过程： 1、恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新），因此每一个mini- batch都在训练不同的网络。 2、从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 3、对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数(w, b)没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 2. 模型介绍及数学原理 前提：带有L层隐藏层的神经网络。 l：第几层； z： 代表输入向量； y： 代表输出向量； W：代表权重； b：偏差； f： 激活函数； 2.1 训练层 在训练网络每一个单元上都加上一道概率流程，从而实现随机删除神经元操作。 详细计算变化如下所示： 原始神经网络前向传播计算公式 zi(l + 1) = wi(l + 1)yl + bi(l + 1), yi(l + 1) = f(zi(l + 1)), 添加Dropout的神经网络前向传播计算公式 rj(l) ∼ Bernoulli(p) ỹ(l) = r(l) * y(l) zi(l + 1) = wi(l + 1)ỹl + bi(l + 1), yi(l + 1) = f(zi(l + 1)) 在训练过程中，虽然停止使用一些神经元，但是整个模型是完整的，为了恢复模型，会对没有被dropout的神经元权值做一个rescale，比如：dropout rate=0.5，经由下式计算处rescale rate=2，从而一定程度上弥补删掉神经元带来的误差 rescale_rate = 1/(1 − dropout_rate) 2.2 测试层 预测模型时，每一个神经单元的权重参数都要乘以概率p 其公式为： w(l) = pWl 权重参数被概率缩放 2.3数学原理 一个线性神经网络，其输出是输入的加权和，此处只考虑简单的线性激活函数，原理同样适用于非线性情况，只是推导更加复杂： 对于无Dropout的网络，误差可以表示如下，其中t是目标值 在加入了Dropout的网络中,满足关系式(8)，所以上式(10)，可以表示如下： 关于Wi的导数如下： 式(3)r ∼ Bernoulli(p)是丢失率，服从伯努利分布。其误差也变成如下所示： 关于Wi的导数如下： 因为ri是一个伯努利分布，所以其期望变为： 对比式(15)和(16)可以看出，在式(8)的前提下，带有Dropout的网络期望等价于带有正则的普通网络。也就是说，Dropout起到正则作用，正则项为Wipi(1 − pi)Ii2 3. 代码实现 3.1 利用Torch实现 利用Torch实现 import torch# 定义dropoutdef dropout_test(x, dropout): \"\"\"\t:param x: input tensor\t:param dropout: probability for dropout\t:return: a tensor with masked by dropout\t\"\"\" # dropout must be between 0 and 1 # 判断dropout是不是在0到1之间，如果不是的话直接抛出异常。 assert 0 = dropout = 1 # if dropout is equal to 0;just return self_x # 如果dropout为0，则返回原式 if dropout == 0: return x # if dropout is equal to 1: put all values to zeros in tensor x # 如果dropout为1，则返回和输入形状相同的全0矩阵。 if dropout == 1: return torch.zeros_like(x) # torch.rand is for return a tensor filled with values from uniform distribution [0,1) # we compare the values with dropout,if values is greater than dropout ,return 1,else 0 # 生成一个mask矩阵，用来使一部分数据为0。 mask = (torch.rand(x.shape) dropout).float() # mask times x and give the scale(1-dropout) for the same expectation with before # 使用mask来遮掉部分数据，并且缩放剩余权重。 return mask * x / (1.0 - dropout)if __name__ == '__main__': # dropout_test input = torch.rand(3, 4) dropout = 0.8 output = dropout_test(input, dropout) print(f\"input={input}\") print(f\"output={output}\") 输出为： input=tensor([[0.9106, 0.2481, 0.5282, 0.4834], [0.4195, 0.0436, 0.3414, 0.6019], [0.6137, 0.8553, 0.6977, 0.6851]])output=tensor([[4.5528, 0.0000, 0.0000, 2.4170], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 4.2766, 0.0000, 0.0000]]) 3.2 利用Numpy实现 3.2.1 测试集不改变 训练模式丢下部分权重，根据式(7)保证其期望不变，这样测试模式值就可以不做额外处理，保持训练与测试期望一致： train_mode: test_mode: 由于测试阶段不适用drop_out，所以为保持训练模式与测试模式期望一致，测试模式下值不变： E(h′) = h′ 实现代码案例如下 在测试集不改变情况下，利用Torch实现 import numpy as npdef another_train(rate, x, w1, b1, w2, b2):\tlayer1 = np.maximum(0, (np.dot(x, w1) + b1))\tmask1 = np.random.binomial(1, 1.0 - rate, layer1.shape)\tlayer1 = layer1 * mask1\tlayer1 = layer1 / (1.0 - rate) layer2 = np.maximum(0, (np.dot(layer1, w2) + b2))\tmask2 = np.random.binomial(1, 1.0 - rate, layer2.shape)\tlayer2 = layer2 * mask2\tlayer2 = layer2 / (1.0 - rate)\treturn layer2def another_test(x, w1, b1, w2, b2):\tlayer1 = np.maximum(0, np.dot(x, w1) + b1)\tlayer2 = np.maximum(0, np.dot(layer1, w2) + b2)\treturn layer2 3.2.2 测试集发生改变 训练模式丢下部分权重，不在训练模式中采用式(7)，而选择在测试模式中乘以(1 − dropout_rate)，也可以保持训练与测试期望一致： train_mode: test_mode: 由于测试阶段不适用drop_out，所以为保持训练模式与测试模式期望一致，测试模式下值不变： E(h′) = h′(1 − p) 实现代码案例如下 在测试集改变情况下，利用Torch实现 import numpy as npdef train(rate, x, w1, b1, w2, b2):\t\"\"\"\tdescription:\tif the train cannot use scale(1/(1-rate)) for output;\tthen we need mutiply by (1.0-rate) for keeping the same expectation\t:param rate: probability of dropout\t:param x: input tensor\t:param w1: weight_1 of layer1\t:param b1: bias_1 of layer1\t:param w2: weight_2 of layer2\t:param b2: bias_2 of layer2\t:return: layer2\t\"\"\"\tlayer1 = np.maximum(0, (np.dot(x, w1) + b1))\tmask1 = np.random.binomial(1, 1.0 - rate, layer1.shape)\tlayer1 = layer1 * mask1\tlayer2 = np.maximum(0, (np.dot(layer1, w2) + b2))\tmask2 = np.random.binomial(1, 1.0 - rate, layer2.shape)\tlayer2 = layer2 * mask2\treturn layer2def test(rate, x, w1, b1, w2, b2):\tlayer1 = np.maximum(0, np.dot(x, w1) + b1)\tlayer1 = layer1 * (1.0 - rate)\tlayer2 = np.maximum(0, np.dot(layer1, w2) + b2)\tlayer2 = layer2 * (1.0 - rate)\treturn layer2 4.注意事项 4.1 防止过拟合方法 当验证集上的效果变差时，提前终止 将L1和L2正则化加权 soft weight sharing dropout 4.2 参考文章"},{"title":"K-近邻算法","path":"/wiki/数据同化与不确定性量化/K-Nearest Neighbours.html","content":"前言 KNN是一个简单的监督学习分类算法，需要有标签的训练数据，通过对新样本距离最近的k个训练样本点，按照分类决策规则决定新样本的类别。于1968年由Cover和Hart提出。 简介 KNN主要有三个要点： 确定距离度量； k值选择 分类决策规则 在分类任务中可使用“投票法”，即选择k个实例中出现最多的标记类别作为预测结果； 在回归任务中可使用“平均法”，即将k个实例的实值输出标记的平均值作为预测结果； 还可基于距离远近使用加权平均或加权投票，距离越近的实例权重越大。 距离度量 特征空间中的两个实例点的距离是两个实例点相似程度的反映。K近邻法的特征空间一般是n维实数向量空间Rn。度量的距离是其他Lp范式距离，一般为欧式距离: 其中，p ≥ 1。 p = 1时，称为曼哈顿距离（Manhattan distance） p = 2时，成为欧式距离（Euclidean distance） p = ∞时，它到各个坐标轴的距离为最大值 k值选择 常用方法： 从k = 1开始，使用检验集估计分类器的误差率； 重复该过程，但k每次加1,允许增加一个近邻； 选取产生最小误差率的k值； 一般k的取值不超过20,上限是n的开方，随着数据集的增大，k值也对应增大； 一般k选取较小的值，并采用交叉验证法选择最优的k值； 经验规则：k一般低于训练样本的平方根； 说明： 过小的k值模型中，输入样本点会对近邻训练样本点十分敏感，如果引入噪声，则会导致预测出错。对噪声的低容忍性使得模型变得过拟合； 过大的k值模型，就相当于选择了范围更大的领域内的点作为决策依据，可以降低估计误差，但领域内的其他类别的样本点也会对该输入样本点的预测产生影响； 如果k = N，N为所有样本点，那么KNN模型每次都会选取训练数据中数量最多的类别作为预测类别，训练数据中的信息将不会被利用； 分类决策规则 一般来说，KNN的分类决策规则就是对输入新本的邻域内所有样本进行统计树木，领域是以新输入样本为中心，离新样本点距离最近的k个点所构成的区域。根据k值决定的邻域，我们可以计算出新样本x*, y*在k值为k*的KNN模型上的预测为第c类的概率： 其中N为邻域内训练样本点的总数目，I(yi = c)是一个指数函数，括号内为真时取1,否则取0。 KNN模型的多数表决规则属于经验风险最小化的策略，因此KNN模型的误分类率为： 算法实现 利用python实现 from numpy import *import operatordef createDataSet(): # 创建训练集 group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group, labels\tdef classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] # 根据欧式距离计算训练集中每个样本到测试点的距离 diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 # 计算完所有点的距离后，对数据按照从小到大的次序排序 sortedDistIndicies = distances.argsort() # 确定前k个距离最小的元素所在的主要分类，最后返回发生频率最高的元素类别 classCount={} for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 其中group和labels为训练集，group为特征向量，labels为类别。 也可以使用sklearn库 利用sklearn库实现 from sklearn.neighbors import KNeighborsClassifierX = [[0], [1], [2], [3]]y = [0, 0, 1, 1]neigh = KNeighborsClassifier(n_neighbors= 3, weights = 'uniform', algorithm = 'auto') # 设置最近的3个邻居作为分类的依据neigh.fit(X, y) # 调用fit()函数，将训练数据X和标签y送入分类器进行学习text = [[1.1], [2.1]] data = neigh.predict(text)print(data)print(neigh.score(X, y)) #返回评分 训练结果： [0 1] 训练结果评分： 1.0 优劣 KNN的主要优点有： 理论成熟，思想简单，既可以用来做分类也可以用来做回归 可用于非线性分类 训练时间复杂度比支持向量机之类的算法低，仅为O(n) 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分 KNN的不足： 计算量大，分类速度慢 KNN在对属性较多的训练样本进行分类时，由于计算量大而使其效率大大降低效果。懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢 K值难以确定 目前没有很好的方法，一般采用先定一个初始值，然后根据实验测试的结果调整K值。 对不平衡样本集比较敏感 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。改进：采用权值的方法（增大距离小的邻居样本的权值）"},{"title":"掩码自动编码器","path":"/wiki/数据同化与不确定性量化/Masked Autoencoders.html","content":"前言"},{"title":"PCA的数学原理","path":"/wiki/数据同化与不确定性量化/PCA的数学原理.html","content":"PCA的数学原理 原链接失效 作者 张洋 | 发布于 2013-06-22 PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。 当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。 数据的向量表示及降维问题 一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下： (日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) 其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子： (500, 240, 25, 13, 2312.15)T 注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。 我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。 降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。 举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。 当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。 这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失 太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。 上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？ 要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。 向量的表示及基变换 既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。 内积与投影 下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为： (a1, a2, …, an)T ⋅ (b1, b2, …, bn)T = a1b1 + a2b2 + … + anbn 内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则，。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图： 好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为|A|cos(a)，其中是向量A的模，也就是A线段的标量长度。 注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。 到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式： A ⋅ B = |A||B|cos(a) 现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让，那么就变成了： A ⋅ B = |A|cos(a) 也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。 基 下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量： 在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。 不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。 更正式的说，向量(x,y)实际上表示线性组合： x(1, 0)T + y(0, 1)T 不难证明所有二维向量都可以表示为这样的线性组合。此处(1, 0)和(0, 1)叫做二维空间中的一组基。 所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1, 0)和(0, 1)为基。 我们之所以默认选择(1, 0)和(0, 1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。 例如，(1, 1)和(−1, 1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为和。 现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐kj标为。下图给出了新的基以及(3,2)在新基上坐标值的示意图： 另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。 基变换的矩阵表示 下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换： 太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示： 于是一组向量的基变换被干净的表示为矩阵的相乘。 一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。 数学表示为： $$ = $$ 其中pi是一个行向量，表示第i个基，aj是一个列向量，表示第j个原始数据记录。 特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。 最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。 协方差矩阵及优化目标 上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？ 要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。 为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式： 其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。 我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后： 我们可以看下五条数据在平面直角坐标系内的样子： 现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？ 通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。 那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。 以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。 下面，我们用数学方法表述这个问题。 方差 上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： 由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： 于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 协方差 对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： 可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 协方差矩阵 上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。 我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感： 假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： 然后我们用X乘以X的转置，并乘上系数1/m： 奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况： 设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 协方差矩阵对角化 根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： 现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足PCPT是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。 至此，我们离“发明”PCA还有仅一步之遥！ 现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。 由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量重数为r，则必然存在r个线性无关的特征向量对应于，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为，我们将其按列组成矩阵： 则对协方差矩阵C有如下结论： 其中为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。 以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P： P = ET P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。 至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 算法及实例 为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 PCA算法 总结一下PCA的算法步骤： 设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P 6）Y = PX即为降维到k维后的数据 实例 这里以上文提到的 为例，我们用PCA方法将这组二维数据其降到一维。 因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵： 然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为： λ1 = 2, λ2 = 2/5 其对应的特征向量分别是： 其中对应的特征向量分别是一个通解，和可取任意实数。那么标准化后的特征向量为： 因此我们的矩阵P是： 可以验证协方差矩阵C的对角化： 最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示： 降维投影结果如下图： 进一步讨论 根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。 因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。 最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。 希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。"},{"title":"图变分自动编码器","path":"/wiki/数据同化与不确定性量化/Variational Graph Auto-Encoders.html","content":"前言 VGAE是一种基于图数据类型的无监督学习方法，参考自动编码器(AE)和变分自动编码器(VAE)结合图数据结构的特点，提出了图自动编码器(GAE)和变分图自动编码器(VGAE)。先简单描述一下图自编码器的intention 和用途：获取合适的 embedding 来表示图中的节点不是容易的事，而如果能找到合适的 embedding，就能将它们用在其他任务中。VGAE 通过 encoder-decoder 的结构可以获取到图中节点的 embedding，来支持接下来的任务，如链接预测等。 介绍 定义 给定一个无向无权图𝒢 = (𝒱, ε)"},{"title":"后验坍塌","path":"/wiki/数据同化与不确定性量化/Posterior Collapse.html","content":"Posterior Collapse定义 在变分自编码器中，目标是最大化变分下界(ELBO)，主要由重构误差和KL散度，两重要部分组成： 常能解析需采样估计 其中常能分析为KL散度（潜在变量分布与先验分布的差异），而后面需采样估计项𝔼qϕ(z|x)[log pθ(x(i)|z)]为重构误差（数据重构的误差）。 后验坍塌常发生在训练过程中，模型的后验分布p(z|x)变得与先验分布p(z)十分接近，也就意味着，模型几乎不利用潜在变量z来解释输入的数据x，从而导致潜在变量在模型生成过程中“崩溃”或“失效”。 Posterior Collapse成因 生成模型缺乏足够学习信号 当生成模型（如解码器）过于强大，能够仅通过输入数据x来生成逼近真实数据的输出，那么潜在变量z的作用就变得不重要，导致潜在空间的维度几乎不会被利用。 训练过程中存在梯度消失或不稳定性： 在一些情况中，梯度消失和不稳定可能导致潜在变量在训练过程中不更新。 过度平滑的先验分布 当先验分布p(z)过于简单（如标准正态分布）时，模型可能学会忽略潜在空间的复杂性，导致后验分布变得不那么有意义。 任务与数据的特性 在一些任务（如简单的图像分类或回归任务）中，模型可能根本不需要潜在变量就能生成足够好的结果，因此会忽略潜在空间。 如何避免Posterior Collapse 调整 KL 散度权重： 在训练过程中，逐渐增加 KL 散度项的权重（例如，通过 KL 散度增益调度器）。这种方法能够让模型初期更多地关注重构损失，之后再引入对潜在空间的正则化，从而避免过早地让后验分布与先验分布接近。 KL Annealing： 逐步增加 KL 散度的权重，让模型可以先专注于生成，后期再引入潜在变量的正则化。 引入更复杂的先验分布： 使用更复杂的先验分布，如 Normalizing Flows（正则化流）或者 Learnable Priors，以避免过于简单的先验导致模型忽略潜在变量。 改进解码器： 增强解码器的能力，让它不仅仅依赖输入数据本身，还要依赖潜在变量z。可以通过更强的生成模型结构来迫使模型利用潜在空间。 使用其他类型的生成模型： 使用如 InfoVAE、VQ-VAE、β-VAE 等模型，它们通过改进潜在空间的结构，能更有效地利用潜在变量，避免 Posterior Collapse 问题。 使用变分目标的改进方法： 使用像 Importance Weighted Autoencoders (IWAE) 等变种方法来改进变分下界的计算，避免直接使用过于简单的变分分布。 更好的初始化和优化策略： 使用适当的初始化和优化策略，以防止训练早期的梯度消失和不稳定，保持潜在变量的有效学习。"},{"title":"Variational Autoencoder","path":"/wiki/数据同化与不确定性量化/Variational Autoencoder.html","content":"Variational Autoencoder(VAE) VAE的首次提出可见参考文献，该论文提出了AEVB（AutoEncoder Variational Bayesian），通过使用SGVB(Stochastic Gradient Variational Bayes)估计器来优化识别模型，利用Kullback - Leibler散度( KLD )对潜变量进行高斯分布约束，以保证潜变量空间几何的光滑性，实现使用简单的原始采样来执行非常有效的近似后验推断，进而允许我们有效地学习模型参数，而不需要每个数据点昂贵的迭代推断方案(如MCMC)。学习到的近似后验推理模型还可以用于许多任务，如识别、去噪、表示和可视化。当神经网络用于识别模型时，我们得出了变分自编码器VAE。 问题分析 使用场景 所使用的策略能够为多中具有连续潜变量的有向图模型推导一个下界估计器，该篇介绍文章不对边缘分布或后验分布作常见的简化假设。相反，提出了一个通用算法，在以下情况中也能高效运行： 不可解释性 即边缘似然pθ(x) = ∫pθ(z)pθ(x|z)dz的积分不可解(无法计算或对其求导)；真实的后验密度（Bayes准则）也不可解(无法使用EM算法)；任何合理的平均场变分贝叶斯算法中所需的积分同样不可解。这类不可解释性在实际中很常见，特别是当似然函数pθ(x|z)稍微复杂时，如包含非线性隐藏层的神经网络； 数据集规模巨大 当数据过多，批量优化的代价过高，希望通过小批量处理更新参数时，基于采样的解决方法（如蒙特卡洛EM）速度太慢，并且需要对每个数据点进行代价高昂的采样循环； 应对方法 参数θ的高效近似最大似然（ML）或最大后验（MAP）估计。参数本身可能就是我们关注的对象，例如在分析某个自然过程时。它们还可以帮助我们模拟隐藏的随机过程，并生成与真实数据相似的人工数据。 在给定观测值x和一组参数θ的条件下，对潜变量z zz进行高效的近似后验推断。这在编码或数据表示任务中非常有用。 对变量x进行高效的近似边缘推断。这使我们能够执行各种推理任务，其中需要对x施加先验。在计算机视觉中的常见应用包括图像去噪、修复和超分辨率等。 变分下界 变分似然由每个数据点的边缘似然之和构成： 重构每一项为： log pθ(x(i)) = DKL(qϕ(z|x(i))∥pθ(z|x(i))) + ℒ(θ, ϕ; x(i)) 第一项为近似后研与真实后验之间的KL散度，由于其非负，第二项被称为数据点i的边缘似然的(变分)下界，可以写作： log pθ(x(i)) ≥ ℒ(θ, ϕ; x(i)) = 𝔼qϕ(z|x)[−log qϕ(z|x) + log pθ(x, z)] 重构成： 常能解析需采样估计 我们希望对变分下界ℒ(θ, ϕ; x(i))分别关于变分参数ϕ和生成模型参数θ求导并优化。然而，这个下界关于ϕ的梯度存在问题，同通常的(朴素)蒙特卡洛梯度估计器时： 其中z(l) ∼ qϕ(z|x(i))。 但是上述的梯度估计器方差非常大，在此次目的中不实用。 详细推导见变分下界(ELBO)推导 重参数化（Reparameterization Trick） 在论文中提出了重参数化（Reparameterization Trick），使用z = gϕ(ε, x)表示隐变量z的生成过程。其中，ε时一个辅助变量，具有独特的已知分布p(ε)。 则有： ∫qϕ(z|x) ⋅ f(z)dz = ∫p(ε) ⋅ f(z)dε = ∫p(ε) ⋅ f(gϕ(ε, x))dε 由于p(ε)的分布已知，因此反向求导参数ϕ的过程是可微且方便计算的。 文中给了如下案例： z ∼ q(z|x) = N(μ; σ) 令z = μ + σε，ε ∼ N(0, 1)，则 𝔼z ∼ N(μ; σ2)[f(z)] = 𝔼ε ∼ N(0, 1)[f(μ + σε)] 详细案例分析可见重参数化简单证明 随机梯度下降变分贝叶斯(SGVB) 后验概率qϕ(z|x)重参数化后z̃ ∼ qϕ(z|x)，重构新的分布gϕ(ε, x)，其噪声变量为ε： z̃ = gϕ(ε, x)，其中ε ∼ p(ε) 由此，其期望的蒙特卡洛估计如下： 应用到式(3)，从而得到随机梯度变分贝叶斯估计器,近似于式(4) 详细推导见变分下界(ELBO)推导 对应于公式(4)，KL散度项可以解释为对ϕ的正则化，鼓励近似后验靠近先验pθ(z)​，此时的第二种SGVB估计器如下，通常比通用估计器具有更小方差 对于包含N个数据点的数据集X，基于小批量构造整个数据集边缘似然下界的估计器： 当小批量大小M足够大时，则每个数据点的采样数L可以设置为。 对关于和求导，得到的梯度可结合随机优化方法使用，下面给出小批量数据版本的自动编码VB ( AEVB )算法: 初始化参数 先随机初始化 θ 和 ϕ。 θ 通常对应解码器网络的参数，ϕ 对应编码器网络的参数。 循环训练（Repeat） 从完整数据集中随机抽取一个小批量（minibatch）。 设数据集大小为 N，每次抽取 M 个数据点（例如 M = 100）。 记这个小批量为 XM = {x(1), x(2), …, x(M)}。 这样做的好处是可以用小批量来近似全数据集的目标函数，减少计算量。 从噪声分布 p(ϵ) 中采样 ϵ。 在 VAE 中，通常取 p(ϵ) 为标准正态分布 𝒩(0, I)。 这些噪声样本会用来做重参数化：z̃ = gϕ(ϵ, x)。 如果你只做一次采样（即 L = 1），就意味着每个数据点 x(i) 只对应一个 ϵ(i) 和一个 z̃(i)。 计算小批量下界的梯度 用公式(8)那样，把变分下界 ℒ(θ, ϕ; X)​ 用小批量+采样来近似： 其中 可以是公式(9)（通用版本）或公式(10)（KL 项可解析时的版本）。 然后对这个小批量近似下界，计算关于 θ 和 ϕ 的梯度： 更新参数 θ 和 ϕ 使用随机梯度下降（SGD）、Adagrad、Adam 等方法，根据上一步的梯度 g 来更新参数： θ ← θ + α ⋅ gθ, ϕ ← ϕ + α ⋅ gϕ. 其中 α 是学习率，或由自适应算法（如 Adam）自动调整。 直到收敛 不断重复上述“小批量采样 + 噪声采样 + 计算梯度 + 参数更新”步骤，直到损失不再显著下降或达到预设的迭代次数。 训练结束后，得到学习到的 θ（解码器）和 ϕ（编码器）。 vae1 VAE VAE是AEVB算法的一个具体例子。AEVB算法中并未对隐变量z的先验pθ(z)及后验qϕ(z|x)做任何约束。 VAE在AEVB算法的基础上做了以下限定： 用encoder神经网络模拟后验概率分布qϕ(z|x)，并假设其满足多元混合高斯： log qϕ(z|x(i)) = log 𝒩(z; μ(i), σ(i)2) 此过程使用两个网络（均值和方差）分别来估计每个样本对应的隐状态z(i)的均值和方差； 假设先验概率pθ(z)满足多元高斯正态分布模型，即：pθ(z) ∼ 𝒩(z; 0, I)； 使用神经网络decoder生成概率模型pθ(x|z)； 使用重采样trick，z(i, l) ∼ qϕ(z|x(i)), z(i, l) = μ(i) + σ(i) ⊙ ε(l), ε(l) ∼ 𝒩(0, I),其中i代表第i个样本，l代表对ε的第l次采样； 使用公式(10)的估计器，对模型数据点x(i)的估计器如下： 高斯分布下 −DKL(qϕ(z)∥pθ(z)) 的解析解D_{KL}(q_(z)||p_(z))$ 变分下界（待最大化的目标函数）包含一个 KL 散度项，该项通常可被解析积分。本节给出当先验分布 pθ(z) = 𝒩(0, I) 和近似后验分布 qϕ(z|x(i)) 均为高斯分布时的解析解。设 z 的维度为 J，μ 和 σ 表示数据点 i 处的变分均值和标准差，μj 和 σj 分别表示这些向量的第 j 个元素。则有： 1. 先验分布的期望 2. 后验分布的熵 3. KL 散度的解析解 说明：当使用识别模型 qϕ(z|x) 时，μ 和 σ 仅是 x 和变分参数 ϕ 的函数。"},{"title":"矢量量化变分自编码器","path":"/wiki/数据同化与不确定性量化/Vector Quantized Variational Autoencoders.html","content":"前言 VQ-VAE更专注于离散表示，这在许多模态中更为自然。如，语言本质上时离散的，语言通常表示为一系列符号，而图像可以由语言来描述，此外，离散表示符合复杂的推理、规划和预测学习。VQ-VAE通过一种新颖的后验分布参数化，成功地把变分自编码器框架与离散潜在表示结合，该模型更依赖于矢量量化（VQ），易于训练，没有大的方差问题，从而避免了许多强大解码器的VAE模型常见的“后验坍塌”问题。 离散隐变量 定义一个潜在嵌入空间e ∈ ℝK × D，其中K是离散潜在空间的大小（即K − way分类），D是每个潜在嵌入向量ei的维度。存在K个嵌入向量ei ∈ ℝD, i ∈ {1, 2, …，K}。 模型结构如上所示。模型接受输入x，通过编码器产生输出ze(x)，离散潜在变量z通过使用共享嵌入空间e进行最近邻查找计算离散潜变量z，公式如下所示： 其中ze(x)是编码器网络的输出，该模型被视为一个VAE，此框架下可以使用ELBO界限log p(x)。后验分类分布q(z = k|x)是确定的，通过定义一个简单的均匀先验分布在z上可以获得一个恒定且等于logK的KL散度。 解码器的输入是对应的嵌入向量ek，如下： zq(x) = ek, 其中k = argminj∥ze(X) − ej∥22 通过离散化瓶颈表示ze(x)，映射到嵌入e的最近元素。 我们可以将前向计算流程视为一个带有特定非线性的常规自编码器，将潜在变量映射到嵌入向量中。 模型学习 公式（2）没有定义真实梯度，实际使用时，只有从解码器输入zq(x)，复制到编码器输出ze(x)。 在前向计算中，最近的嵌入zq(x)被传递给解码器，在后向传递中，梯度∇zL被传递给编码器。由于编码器输出表示与解码器输入共享相同的D维空间，梯度包含了编码器如何改变其输出以降低重构损失的有用信息。由于公式（1）中的赋值，梯度可以推动编码器的输出在下一个前向通道中被不同的离散化。总体的损失函数如下所示： ℒ = log p(x|zq(x))+∥sg[ze(x)] − e∥22 + β∥ze(x) − sg[e]∥22 其由三个部分组成，分别用于训练VQ-VAE的三个组成部分： log p(x|z1(x)) 第一项是重构损失(或数据项)，优化了解码器和编码器(通过上述解释的估计量)。由于ze(x)到zq(x)映射的直通梯度估计，嵌入ei没有从重构损失logp(z|zq(x))中获得梯度。解码器仅优化这一项 ∥sg[ze(x)] − e∥22 矢量量化（VQ），使用l2误差将嵌入向量ei向编码器输出ze(x)移动，这个损失项仅用于更新字典，所以可以选择使用ze(x)的移动平均来更新字典项。此项优化嵌入 VQ - VAE字典采用指数移动平均法进行更新: 设zi，1，zi，2，..，zi，ni是编码器输出的ni个最靠近字典项ei​的集合，这样我们就可以把损失写成： ei的最优值有一个闭式解，简单来说就是集合中元素的平均值: 这种更新通常用于K - Means等算法中。 但是，在处理小批量时，我们不能直接使用这个更新。相反，我们可以使用指数移动平均线作为这个更新的在线版本： 其中γ ∈ (0, 1)，当γ = 0.99时，实际工作效果最好。 β∥ze(x) − sg[e]∥22 承诺损失，确保编码器对嵌入做出承诺，其输出不会增长。编码器优化此项和第一项，β具有很高鲁棒性，在0.1-2.0范围内变化不大。 其中，sg表示停止梯度操作符，在前向计算时定义为恒等操作，在偏导数为零时有效将其操作数约束为不更新的常数。 完整模型的对数似然 log p(x) = log ∑kp(x|zk)p(zk) 因为解码器p(x|z)使用来自最大后验推断的z = zq(x)进行训练，一旦z ≠ zq(x)完全收敛时，解码器不应该将任何概率质量分配给p(x|z)。因此可以写成 log p(x) ≈ log p(x|zQ(x))p(zq(x)) 根据詹森不等式，也可写作log p(x) ≥ log p(x|zq(x))p(zq(x))。 先验分布 离散潜变量z上的先验分布p(z)是一个分类分布，可以通过依赖特征映射中的其他z来使其成为自回归。在训练VQ-VAE期间，先验保持恒定且均匀。训练完成后，作者拟合了一个自回归分布p(z)，以便通过祖先采样生成x。对于图像，使用PixelCNN来处理离散潜变量；对于原始音频，使用WaveNet。 代码实现 可参考非官方代码"},{"title":"变分下界(ELBO)推导","path":"/wiki/数据同化与不确定性量化/变分下界(ELBO)推导.html","content":"边缘似然的分解 从单数据点x(i)的边缘似然log pθ(x(i))出发，引入变分分布qϕ(z|x(i))进行分解： log pθ(x(i)) = DKL(pϕ(z|x(i))∥pθ(z|x(i))) + ℒ(θ, ϕ; x(i)) 推导步骤： 引入变分分布 对任意分布qϕ(z|x)有： log pθ(x) = log ∫pθ(x, z)dx 分子分母技巧 期望的积分形式 𝔼qϕ(z|x)[f(z)] = ∫f(z)qϕ(z|x)dz 应用琴生不等式(Jensen’s Inequality) f(x)需要为凹函数： 因此： ≜表示定义为的意思； 定义KL散度 结合贝叶斯定理： 贝叶斯定理表达式如下： 即联合分布pθ(x, z)可写作： pθ(x, z) = pθ(z|x) ⋅ pθ(x) 变分下界定义如下： 由于log pθ(x)不依赖于z，所以期结果为： 引入KL散度： 因此： ℒ(θ, ϕ; x) = log pθ(x) − DKL(qϕ(z|x)∥pθ(z|x)) 移项得到 log pθ(x) = ℒ(θ, ϕ; x) + DKL(qϕ(z|x)∥pθ(z|x)) 根据式(5)： 核心公式： 可优化项 KL散度非负性：保证ℒ是log p(x)的下界； 变分推断的目标：最大化ℒ，间接最小化DKL； 变分下界ℒ的两种形式 形式1：原始定义 ℒ(θ, ϕ; x(i)) = 𝔼qϕ(z|x)[−log qϕ(z|x) + log pθ(x, z)] 形式2：分解KL散度与重构项 ℒ(θ, ϕ; x(i)) = −DKL(qϕ(z|x(i))∥pθ(z)) + 𝔼qϕ(z|x)[log pθ(x(i)|z)] 推导步骤 展开联合分布 将pθ(x, z)分解为似然和先验： log pθ(x, z) = log pθ(x|z) + log pθ(z) 带入形式1 ℒ = 𝔼qϕ(z|x)[−log qϕ(z|x) + log pθ(x|z) + log pθ(z)] 重组项 ℒ = 𝔼qϕ(z|x)[log pθ(x|z)] + 𝔼qϕ(z|x)[log pθ(z) − log qϕ(z|x)] 定义KL散度 𝔼qϕ(z|x)[log pθ(z) − log qϕ(z|x)] = −DKL(qϕ(z|x)||pθ(z)) 梯度估计器的高方差问题 梯度估计器 目标：计算∇ϕℒ假设ℒ包含期望项 ℒ = 𝔼qϕ(z|X)[f(x)] 我们需要计算∇ϕ𝔼qϕ(z|X)[f(x)] 期望积分： 𝔼qϕ(z|x)[f(z)] = ∫f(z)qϕ(z|x)dz 对ϕ求导 ∇ϕ𝔼qϕ(z|x)[f(z)] = ∇ϕ∫f(z)qϕ(z|x)dz 在温和条件下（如积分和导数均收敛），可以进行交换 ∇ϕ𝔼qϕ(z|x)[f(z)] = ∫f(z)∇ϕqϕ(z|x)dz 对数求导技巧： 对任意概率密度qϕ(z|x)有 ∇ϕqϕ(z|x) = qϕ(z|x) ⋅ ∇ϕlog qϕ(z|x) 运用对数求导技巧 ∫f(z)∇ϕqϕ(z|x)dz = ∫f(z) ⋅ qϕ(z|x) ⋅ ∇ϕlog qϕ(z|x)dz 等价于 ∫f(z) ⋅ qϕ(z|x) ⋅ ∇ϕlog qϕ(z|x) dz = ∫qϕ(z|x) ⋅ f(z) ⋅ ∇ϕlog qϕ(z|x) dz = 𝔼qϕ(z|x)[f(z)∇ϕlog qϕ(z|x)] 故而基于评分函数估计器(Score Function Estimator)的朴素蒙特卡洛梯度估计器： ∇ϕ𝔼qϕ(z|x)[f(z)] = 𝔼qϕ(z|x)[f(z)∇ϕlog qϕ(z|x)] 蒙特卡洛近似： 采样z(l) ∼ qϕ(z|x)，估计梯度： 高方差原因 原因一 评分函数的性质：∇ϕlog qϕ(z|x) 的幅度与概率密度成反比。当 qϕ(z|x) 接近 0 时，梯度会剧烈震荡。 数学原理 设变分分布为 $ q_(z|x) ，其评分函数（ScoreFunction）定义为对数概率密度的梯度：$ q(z|x) 对任意分布，以下等式成立： q(z|x) = q_(z|x) q(z|x) $$ 因此： 显然，当 时，分母趋近于零，梯度幅度 $ |q(z|x)| $ 会急剧增大。 直观解释 以高斯分布 为例： 概率密度函数： 评分函数（对 求导）： 当 z 远离均值 μ（即处于分布尾部）时，q(z) 的值很小（概率密度低），但评分函数的值 会很大。 若 z 距离 μ 非常远（如 z = μ + 10σ），则 q(z) ≈ 0 但评分函数值为 此时梯度幅度极大，导致采样估计的剧烈震荡。 原因二 乘积项的方差：f(z)∇*ϕlog q*ϕ(z|x) 的乘积会放大采样噪声。 数学原理 梯度估计器的形式为： 其中 z(l) ∼ qϕ(z|x)。 方差来源 f(z) 的波动：若 f(z) 在不同样本 z 上差异较大（如某些区域 f(z) 值很大），会直接放大梯度的波动。 评分函数的波动：如前述，评分函数在低概率区域会产生极大值。 两者的乘积会导致方差叠加，进一步放大噪声。 直观案例 假设 qϕ(z|x) 是高斯分布 𝒩(μ, σ2)，则： 当 σ → 0 时，梯度项 的方差趋于无穷大。 总结 变分下界通过引入变分分布和KL散度，将边缘似然分解成下界和KL项，下界可以进一步分解为负KL散度和重构期望；而基于评分函数的梯度估计器在理论上可行，但是过于依赖概率密度的倒数，导致出现高方差问题，想要解决这一问题，可以使用重参数化技巧（Reparameterization Trick），将随机性转移到外部变量而降低方差。"},{"title":"自编码器前言","path":"/wiki/数据同化与不确定性量化/index.html","content":"AE基本概念 基于机器学习的降阶模型主要是自编码器（AutoEncoder）,其发展历程图大致如下，出自《Machine Learning With Data Assimilation and Uncertainty Quantification for Dynamical Systems: A Review》： 自动编码器是一种特定类型的自监督神经网络，具有相同的输入和输出。一个典型的自编码器由一个编码器E和一个解码器D组成，编码器E将输入变量映射到约化的隐空间，解码器D从隐表示中重构全物理场，即: zt = E(xt) xtAE = D(zt) 编码器E和解码器D以最小化重构损失为目标进行联合训练，例如用均方误差( MSE )来量化: ℒMSE = 𝔼(∥xt − D ∘ E(xt∥F2) 其中，ℒ是损失函数，∥⋅∥是范德蒙式，𝔼是期望算子。 对比于基于投影的降维方法如POD，自编码器(AE)在处理非线性模型方面具有巨大潜力。但其可解释较差，有许多科研人员在AE的基础上进行改进，如结合传统降维方法（如主成分分析等），下面将介绍前言中发展历程的主要算法。"},{"title":"广义本征分解","path":"/wiki/数据同化与不确定性量化/广义本征分解.html","content":"广义本征分解（Proper Generalized Decomposition，PGD） 背景 广义本征分解（Proper Generalized Decomposition，PGD）是一种高效求解高维问题的降阶方法，特别适合处理多参数、高维度的偏微分方程（PDE）。与POD（数据驱动）不同，PGD是先验型方法，无需预计算完整解，直接构建解的低维表示。核心思想是将高维解表示为分离变量的乘积形式： 其中 x 是空间变量，t 是时间，μ 是参数。PGD通过贪婪算法逐项构建解，显著降低计算复杂度。 数学原理 主要分为三个步骤： 解的低维表示 假设解可分离为： 对每个模态 Xi(x)Ti(t)，通过交替迭代求解。 迭代格式(固定点迭代) 以第 n 项为例： 初始化：随机生成 Tn(0)(t) 固定 Tn，求 Xn​： ∫Ω(ℒ(XnTn) − f)Tndx = 0 固定 Xn，求 Tn： ∫t(ℒ(XnTn) − f)Xndt = 0 重复2-3步直到收敛 弱形式与离散化 以热方程为例： 弱形式离散后得到线性系统： AxXT⊤ + XAtT⊤ = F 其中 Ax 是空间刚度矩阵，At 是时间质量矩阵。 特性 PGD POD 维度处理 直接处理高维参数空间 需预计算全解 计算成本 离线计算复杂，在线快速 离线快，在线需投影 数据依赖 无需求解完整解（先验方法） 需要快照数据（后验） 适用场景 参数化偏微分问题（PDE）、实时控制 固定参数瞬态问题"},{"title":"本征正交分解","path":"/wiki/数据同化与不确定性量化/本征正交分解.html","content":"本征正交分解(proper orthogonal decomposition,POD) 本征正交分解（Proper Orthogonal Decomposition, POD），亦被称为主成分分析（Principal Component Analysis, PCA），在样本识别领域，被称为Karhunen-Loeve(K-L)展开方法。旨在从一组庞大的实验或模拟数据中获取一组低维最优基，然后用这组最优基来尽可能逼近原来的复杂系统。 选用不同坐标系中，坐标的信息量在计算机中所占内存不同。以笛卡尔坐标系中在一条直线上的三个点为例：坐标分别为：A(1, 1)、B(2, 2)、C(3, 3)，转换成ox″坐标系（以直线y = x为轴），信息变为、、，原本需要六个数据降低到三个数据，从二维降维到一维。 数学原理 详细见张洋发布于 2013-06-22的博客内容，因原网址失效，个人进行拷贝修复，详细如下： 直接POD方法 定义 固定初始条件u0 ∈ RN，给定0 ≤ t ≤ T的解ut，本征正交分解方法寻找一个固定秩k的正交投影ΠV, V，以最小化积分投影误差。 J(ΠV, V) = ∫0T∥u(t) − ΠV, Vu(t)∥22dt = ∫0T∥εV⟂(t)∥22dt 定义对称正定矩阵K̂ ∈ RN × N K̂ = ∫0Tu(t)u(t)Tdt k̂的特征值为λ̂1 ≥ λ̂2 ≥ … ≥ λ̂n ≥ 0，对应的特征向量为ϕ̂ ∈ RN,满足： K̂ϕ̂i = λ̂iϕ̂i, i = 1, 2, …, N 假设λ̂k λ̂k + 1，那么使J(ΠV, V)达到最小值的子空间,是矩阵K̂关于特征值λ̂1, λ̂2, …, λ̂k的不变子空间。 适用于采样空间点数m小于快照个数N的情形。处理数据量过小，基本不使用，当维度过大时，一般使用下面的快照POD方法。 快照POD(Snapshots POD) 一般情况下，求解特征值问题K̂ϕ̂i = λ̂iϕ̂i在计算上难以处理，基于以下原因： 矩阵K̂的维度N通常很大 矩阵K̂通常是稠密矩阵（该矩阵特点是举证中大部分或全部元素为非零值） 状态数据通常以离散的向量形式(snapshot)提供： {u(ti)}i = 1Nsnap 在这种情况下∫0Tu(t)u(t)Tdt可以利用数值积分规则进行近似： 定义快照矩阵S ∈ RN × Nsnap​ 那么 K = SST 目标变为：近似K的前k个特征值和特征向量。 定义R = SST ∈ RNsnap × Nsnap，假设Rank(R) = r ≤ Nsnap，求解特征值问题有： Rψi = λiψi i = 1, 2, …, r 那么K = SST的前r个特征向量，即POD 的模态满足： 复杂度为：𝒪(Nsnap2N) 条件数(comdition number)：cond(R) ≈ cod(S)2 基于SVD的POD方法 SVD(Singular Value Decomposition,奇异值分解)可以看作是非方阵情况下特征值分解的拓展。 奇异值分解 给定矩阵A ∈ RN × M，存在两个正交矩阵，U ∈ RN × N(UUT = I), V ∈ RM × M(VVT = I)，使得 A = U∑VT 其中∑ ∈ RM × N = diag{σ1, σ2, …, σmin{M, N} 为对角矩阵，满足： σ1 ≥ σ2 ≥ … ≥ σmin{M, N} ≥ 0 那么{σi}i = 1min{M, N}是矩阵A的奇异值，而U和V的列分别是矩阵A的左奇异向量和右奇异向量 U = [u1 u2 …… uN] V = [v1 v2 …… vN] 奇异值分解性质 Rank(A) = r本征正交分解(proper orthogonal decomposition,POD) 本征正交分解（Proper Orthogonal Decomposition, POD），亦被称为主成分分析（Principal Component Analysis, PCA），在样本识别领域，被称为Karhunen-Loeve(K-L)展开方法。旨在从一组庞大的实验或模拟数据中获取一组低维最优基，然后用这组最优基来尽可能逼近原来的复杂系统。 选用不同坐标系中，坐标的信息量在计算机中所占内存不同。以笛卡尔坐标系中在一条直线上的三个点为例：坐标分别为：A(1, 1)、B(2, 2)、C(3, 3)，转换成ox″坐标系（以直线y = x为轴），信息变为、、，原本需要六个数据降低到三个数据，从二维降维到一维。 数学原理 详细见张洋发布于 2013-06-22的博客内容，因原网址失效，个人进行拷贝修复，详细如下： 直接POD方法 定义 固定初始条件u0 ∈ RN，给定0 ≤ t ≤ T的解ut，本征正交分解方法寻找一个固定秩k的正交投影ΠV, V，以最小化积分投影误差。 J(ΠV, V) = ∫0T∥u(t) − ΠV, Vu(t)∥22dt = ∫0T∥εV⟂(t)∥22dt 定义对称正定矩阵K̂ ∈ RN × N K̂ = ∫0Tu(t)u(t)Tdt k̂的特征值为λ̂1 ≥ λ̂2 ≥ … ≥ λ̂n ≥ 0，对应的特征向量为ϕ̂ ∈ RN,满足： K̂ϕ̂i = λ̂iϕ̂i, i = 1, 2, …, N 假设λ̂k λ̂k + 1，那么使J(ΠV, V)达到最小值的子空间,是矩阵K̂关于特征值λ̂1, λ̂2, …, λ̂k的不变子空间。 适用于采样空间点数m小于快照个数N的情形。处理数据量过小，基本不使用，当维度过大时，一般使用下面的快照POD方法。 快照POD(Snapshots POD) 一般情况下，求解特征值问题K̂ϕ̂i = λ̂iϕ̂i在计算上难以处理，基于以下原因： 矩阵K̂的维度N通常很大 矩阵K̂通常是稠密矩阵（该矩阵特点是举证中大部分或全部元素为非零值） 状态数据通常以离散的向量形式(snapshot)提供： {u(ti)}i = 1Nsnap 在这种情况下∫0Tu(t)u(t)Tdt可以利用数值积分规则进行近似： 定义快照矩阵S ∈ RN × Nsnap 那么 K = SST 目标变为：近似K的前k个特征值和特征向量。 定义R = SST ∈ RNsnap × Nsnap，假设Rank(R) = r ≤ Nsnap，求解特征值问题有： Rψi = λiψi i = 1, 2, …, r 那么K = SST的前r个特征向量，即POD 的模态满足： 复杂度为：𝒪(Nsnap2N) 条件数(comdition number)：cond(R) ≈ cod(S)2 基于SVD的POD方法 SVD(Singular Value Decomposition,奇异值分解)可以看作是非方阵情况下特征值分解的拓展。 奇异值分解 给定矩阵A ∈ RN × M，存在两个正交矩阵，U ∈ RN × N(UUT = I), V ∈ RM × M(VVT = I)，使得 A = U∑VT 其中∑ ∈ RM × N = diag{σ1, σ2, …, σmin{M, N} 为对角矩阵，满足： σ1 ≥ σ2 ≥ … ≥ σmin{M, N} ≥ 0 那么{σi}i = 1min{M, N}是矩阵A的奇异值，而U和V的列分别是矩阵A的左奇异向量和右奇异向量 U = [u1 u2 …… uN] V = [v1 v2 …… vN] 奇异值分解性质 Rank(A) = r，其中r是最小非零奇异值的序号。 定义非零奇异值∑r = diag{σ1, σ2, …, σr}的左、右奇异向量 Ur = [u1 u2 …… ur] Vr = [v1 v2 …… vr] 以及 UN − r = [ur + 1 ur + 2 …… uN] VN − r = [vr + 1 vr + 2 …… vN] 那么 给定上式我们能得到AAT和ATA的特征值分解 它们的非零特征值为σ12, σ22, …, σr2 对应于AAT的特征向量为Ur = [u1, u2, …, ur] 对应于ATA的特征向量为Vr = [v1, v2, …, vr] 给定矩阵A ∈ RN × M，以及k r = rank(A) 其中， 给定快照矩阵S ∈ RN × Nsnap的奇异值分解S = Ur∑rVrT 那么K̂的特征值分解满足 正交投影：V由Ur的前k列组成 复杂度为：𝒪(Nsnap2N) 条件数(comdition number)：cod(S) matlab实现 见这篇博客，个人觉得将的很详细，通俗易懂 https://blog.csdn.net/weixin_42943114/article/details/106338530?fromshare=blogdetailsharetype=blogdetailsharerId=106338530sharerefer=PCsharesource=qq_64820731sharefrom=from_link ，其中r是最小非零奇异值的序号。 定义非零奇异值∑r = diag{σ1, σ2, …, σr}​的左、右奇异向量 Ur = [u1 u2 …… ur] Vr = [v1 v2 …… vr] 以及 UN − r = [ur + 1 ur + 2 …… uN] VN − r = [vr + 1 vr + 2 …… vN] 那么 给定上式我们能得到AAT和ATA的特征值分解 它们的非零特征值为σ12, σ22, …, σr2 对应于AAT的特征向量为Ur = [u1, u2, …, ur] 对应于ATA的特征向量为Vr = [v1, v2, …, vr] 给定矩阵A ∈ RN × M，以及k r = rank(A) 其中， 给定快照矩阵S ∈ RN × Nsnap的奇异值分解S = Ur∑rVrT 那么K̂的特征值分解满足 正交投影：V由Ur的前k列组成 复杂度为：𝒪(Nsnap2N) 条件数(comdition number)：cod(S) matlab实现 见这篇博客，个人觉得将的很详细，通俗易懂 https://blog.csdn.net/weixin_42943114/article/details/106338530?fromshare=blogdetailsharetype=blogdetailsharerId=106338530sharerefer=PCsharesource=qq_64820731sharefrom=from_link"},{"title":"重参数化简单证明","path":"/wiki/数据同化与不确定性量化/重参数化简单证明.html","content":"步骤 1: 写出期望的积分形式 左边（对 z 的期望）： 其中这里是正态分布的概率密度函数（）。 右边（对 ε 的期望）： 其中 步骤 2: 应用变量变换 z = μ + σε 变换关系： z = gϕ(ε, x) = μ + σε 这是一个线性变换，可逆（因为 σ ≠ 0），逆变换为 。 计算雅可比行列式： 变换的雅可比矩阵是标量（一维情况），导数 。 雅可比行列式的绝对值是 （假设 σ 0，所以 |σ| = σ）。 概率密度变换规则： 根据变量变换定理，z 的密度 q(z) 和 ε 的密度 p(ε) 满足： 因为 。 代入 p(ε)： 但 ，所以： 这与给定的 q(z) 一致，验证了变换的正确性。 步骤 3: 变换积分 现在，将左边积分 ∫f(z)q(z)dz 转换为 ε 空间： 由 z = μ + σε，得 dz = σdε（因为 ）。 代入积分： ∫−∞∞f(z)q(z)dz = ∫−∞∞f(μ + σε)q(μ + σε) ⋅ σdε 这里： f(z) 替换为 f(μ + σε)。 q(z) 替换为 q(μ + σε)。 dz 替换为 σdε。 计算 q(μ + σε)： 由 q(z) 的定义： 而 ，所以： 代入积分： 简化： （因为） 结果： ∫f(z)q(z)dz = ∫f(μ + σε)p(ε)dε 这正是： 𝔼z ∼ 𝒩(μ, σ2)[f(z)] = 𝔼ε ∼ 𝒩(0, 1)[f(μ + σε)]"},{"title":"卷积自编码器","path":"/wiki/数据同化与不确定性量化/Convolutional Autoencoder.html","content":"前言 其实现过程与AutoEncode思想一致，喜爱年编码再解码，比较解码数据与原始数据的差异进行训练，最后得到较为稳定的参数，待这一层参数都训练好后再进行下一个训练，CAE主要是加入了卷积的训练，对高维输入具有良好伸缩性的分层无监督特征提取器。使用简单的随机梯度下降学习非平凡的特征，并具有良好的CNNs初始化，避免了几乎所有深度学习问题中出现的高度非凸目标函数的许多独特的局部极小值。 关键数学计算 卷积层-卷积 初始化k和卷积和（W），每个卷积核搭配一个偏置b，与输入x卷积后生成k个特征图h，激活函数如下： hk = σ(x * Wk + bk) 池化层-池化操作（Max Pooling） 对上面生成的特征图进行池化操作，保留池化时的位置关系矩阵，方百年后续反池化操作； 池化层-自编码（反池化操作） 对上面生成的特征图进行反池化操作，用到保留池化时的位置关系的矩阵，将数据还原到原始大小矩阵的相应位置（参考卷积神经网络的过程）； 卷积层-自编码（反卷积操作） 每张特征图h与对应的卷积核的转置进行卷积操作并将结果求和，然后加上偏置c，激活函数公式如下： y = σ(∑k ∈ Hhk − W̃k + c) 其中，H识别潜在特征图组；W̃标识了对权重两个维度的翻转操作,式（1）（2）中的卷积根据实际情况所定。 卷积层-更新权值 最小化的代价函数是均方误差( MSE )，公式如下： 向传播算法用于计算误差函数关于参数的梯度。这可以很容易地通过卷积操作使用下面的公式得到： δh和δy分别是隐状态和重构的延迟。随后使用随机梯度下降更新权重。 卷积的三种操作: 假设原始图像的尺寸为“n × n”，卷积核的尺寸为“m × m”，在进行卷积操作时，对于边界如果需要进行边界补零操作 vaild 得到的最终图像尺寸为“(n − m + 1) × (n − m + 1)” full 得到的最终图像尺寸为“(n + m − 1) × (n + m − 1)” same 得到的最终图像尺寸为“n × n” 最大池化(Max Pooling) 引入Max Pooling下采样技术，获取特征的平移不变性。最大池化通过一个常数因子对潜在表示进行下采样，通常取非重叠子区域上的最大值。这有助于提高滤波器的选择性，因为潜在表示中每个神经元的激活是由感兴趣区域上的特征和输入场之间的”匹配”决定的。最大池化最初只针对强监督前馈架构。具体操作就是在Pooling时选取最大值，如果采用了pooling技术，就不需要使用L1或L2正则化。 代码解析 目前在github上寻找到一个使用python编写的程序”convolutional_autoencoder“，暂时还在研读 由于关于CAE的相关资料太少，有关推导也不足，有待继续完善，为完待续……"}]